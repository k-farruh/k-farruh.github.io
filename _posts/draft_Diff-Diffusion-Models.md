---
title: 'Sion Models For Image Generation'
date: draft_Diff
permalink: /posts/draft_D/Diffusion-Models/
tags:
  - cool posts
  - category1
  - category2
---

Diffusion Models for Image Generation
 \* {
 font-family: Georgia, Cambria, "Times New Roman", Times, serif;
 }
 html, body {
 margin: 0;
 padding: 0;
 }
 h1 {
 font-size: 50px;
 margin-bottom: 17px;
 color: #333;
 }
 h2 {
 font-size: 24px;
 line-height: 1.6;
 margin: 30px 0 0 0;
 margin-bottom: 18px;
 margin-top: 33px;
 color: #333;
 }
 h3 {
 font-size: 30px;
 margin: 10px 0 20px 0;
 color: #333;
 }
 header {
 width: 640px;
 margin: auto;
 }
 section {
 width: 640px;
 margin: auto;
 }
 section p {
 margin-bottom: 27px;
 font-size: 20px;
 line-height: 1.6;
 color: #333;
 }
 section img {
 max-width: 640px;
 }
 footer {
 padding: 0 20px;
 margin: 50px 0;
 text-align: center;
 font-size: 12px;
 }
 .aspectRatioPlaceholder {
 max-width: auto !important;
 max-height: auto !important;
 }
 .aspectRatioPlaceholder-fill {
 padding-bottom: 0 !important;
 }
 header,
 section[data-field=subtitle],
 section[data-field=description] {
 display: none;
 }
 

Diffusion Models for Image Generation
=====================================




Generative AI models aim to generate realistic and diverse data samples. While many successful generative models suffer from instabilityâ€¦




---

### Diffusion Models for Image Generation

Generative AI models aim to generate realistic and diverse data samples. While many successful generative models suffer from instability during training and mode collapse, where the model generates limited and repetitive samples. The Stable Diffusion Model (SDM) is a generative model that addresses these issues by incorporating a diffusion process. Here we introduce the overall Diffusion Model, how it works, the differential with other models, why it addresses the above issues, and its advantages over the before the era of the stable diffusion generative models.

### Overview of the Stable Diffusion Model

The SDM is a generative model that employs a diffusion process to generate samples. The diffusion process involves iteratively applying noise to the data, making it more and more diffuse. This process creates a continuum of intermediate states that are used to generate samples. The SDM is trained to invert this diffusion process to generate samples from the intermediate states.

The SDM is a stochastic model, which means that it generates samples by drawing random samples from a probability distribution. The probability distribution is defined by a series of Gaussian distributions that are parameterized by a neural network. The neural network learns to adjust the parameters of the Gaussian distributions to generate samples that match the input data.

#### Unique Properties of the Stable Diffusion Model

The SDM has unique properties that distinguish it from other generative models:

* The SDM generates samples through a diffusion process, which makes it more stable during training. The diffusion process adds noise to the data, making it more robust to overfitting and less prone to mode collapse.
* The SDM generates samples from a continuum of intermediate states rather than discrete samples. This makes it easier to generate diverse samples that are not limited to a specific set of modes.
* The SDM can be controlled by adjusting the diffusion process. By controlling the rate of diffusion, the SDM can generate samples that are more or less similar to the input data. This allows users to generate more or fewer samples similar to the input data, depending on their needs.

### Advantages of the Stable Diffusion Model

Letâ€™s see what the core advantages SDM has over existing generative models are:

1. The SDM is more stable during training, making it less prone to mode collapse. This means the SDM can generate more diverse samples and avoid generating repetitive ones.
2. The SDM generates samples from a continuum of intermediate states, making it easier to generate diverse samples not limited to a specific set of modes. This allows users to generate more diverse and interesting samples that are not limited to a specific set of modes.
3. The SDM can be controlled by adjusting the diffusion process, which makes it more flexible and adaptable to different use cases. This allows users to generate more or fewer samples similar to the input data, depending on their needs.

Examples

To illustrate the capabilities of the SDM, we will provide examples of its output. The first example is a set of images of faces generated by the SDM. The SDM was trained on a dataset of faces and was able to generate realistic and diverse samples that captured the variability of the input data.

The second example is a set of images of flowers generated by the SDM. The SDM was trained on a flower dataset and could generate samples that capture the shape and color of the input data.

### Conclusion

The Stable Diffusion Model is a powerful generative model that incorporates a diffusion process to generate samples. The diffusion process makes the SDM more stable during training and less prone to mode collapse. The SDM generates samples from a continuum of intermediate states, making generating diverse and interesting samples easier. Finally, the SDM can be controlled by adjusting the diffusion process, making it more flexible and adaptable to different use cases.

  


Generative modeling defines how a dataset is generated. It tries to understand the distribution of data points, providing a model of how the data is generated using a probabilistic model. (e.g., support vector machines or the perceptron algorithm gives a separating decision boundary, but no model for generating synthetic data points). The aim is to generate new samples from what has already been distributed in the training data.

Assume you have an autonomous driving dataset with an urban-scene setting. You now want to generate images from it that are semantically and spatially similar. This is a perfect example of a generative modeling problem. To do this, the generative model must understand the dataâ€™s underlying structure and learn the realistic, generalized representation of the dataset, such as the sky is blue, buildings are usually tall, and pedestrians use sidewalks.

Diffusion models in deep learning were first introduced by S[oul-Dickstein et al. in the original 2015 paper â€œDeep Unsupervised Learning Using Non-Equilibrium Thermodynamicsâ€](https://arxiv.org/abs/1503.03585). Unfortunately, this remained behind the scenes for a while.

But in 2019, [Song et al. published a paper called Generative Modeling by Estimating Data Distribution Gradients](https://arxiv.org/abs/1907.05600) using the same principle but a different approach. In 2020, [Ho et al. published the currently popular paper â€œProbabilistic Noise Diffusion Modelsâ€](https://arxiv.org/abs/2006.11239) (abbreviated as DDPM).

After 2020, research on diffusion models has begun ğŸš€. In a relatively short time, significant progress has been made in building, training, and improving diffusion-based generative modeling.

  


Direct Distribution:

The original image (x 0) is slowly iteratively distorted (Markov chain) by adding (scaled Gaussian) noise.

This process is performed for some time steps T, i.e. xT.

Image at time step t is created: xt-1 + Îµt-1 (noise) â†’ xt

At this stage, the model is not involved.

At the end of the direct diffusion step Xt, due to the iterative addition of noise, we are left with a (clean) noisy image representing an â€œisotropic Gaussianâ€. Itâ€™s just a mathematical way of saying that we have a standard normal distribution and the variance of the distribution is the same across all dimensions. We have converted the distribution of the data to a Gaussian distribution.

Reverse / Reverse Diffusion:

At this point, we cancel the forwarding process. The challenge is to remove the noise added in the direct process again in an iterative way (Markov chain). This is done using a neural network model.

The task of the model is as follows: Given a time interval t and a noisy image xt, predict the noise (Î­) added to the image at step t-1.

xt â†’ Model â†’ Î­ (predicted noise). The model predicts (approximates) the noise added to x t-1 in the forward pass.

  


<https://github.com/Stability-AI/stablediffusion>

  


  


Stable Diffusion  
Created by StabilityAI. Stable Diffusion builds on high resolution image synthesis work using latent diffusion models by Rombach et al. It is the only diffusion based imaging model on this list that is completely open source.

As of this writing, Stable Diffusion v2.1 is available in the official StabilityAI repository.

Not only that, the open source developer community has been very active since its release. In a short period of time, the community has released several stable open source diffusion models, fine-tuned for various artistic styled datasets. You can freely use these models and create new images using these styles.

They can range from Japanese anime and futuristic robots to cyberpunk worlds. Just to intrigue your imagination, here are some examples of stable diffusion models.

The complete architecture of stable diffusion consists of three models:

A text encoder that accepts a text prompt.

Convert text hints to machine-readable vectors.

U-net

This is a diffusion model responsible for generating images.

A variational autoencoder consisting of an encoder and decoder model.

An encoder is used to reduce the size of an image. The UNet diffusion model works in this smaller dimension.

The decoder is then responsible for enhancing/restoring the image generated by the diffusion model to its original size.

One can easily access stable diffusion models using their DreamStudio platform. Creating an account will initially grant you 200 credits which you can use to play with the hints and create images of your choice.

Also, if you have the computing resources, you can also set up stable diffusion to run on your system by following the documentation in their repository.

<https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer>

[**Lilâ€™Log**](https://lilianweng.github.io/ "Lil'Log (Alt + H)")

* [Posts](https://lilianweng.github.io/ "Posts")
* [Archive](https://lilianweng.github.io/archives "Archive")
* [Search](https://lilianweng.github.io/search/ "Search (Alt + /)")
* [Tags](https://lilianweng.github.io/tags/ "Tags")
* [FAQ](https://lilianweng.github.io/faq "FAQ")
* [emojisearch.app](https://www.emojisearch.app/ "emojisearch.app")

### What are Diffusion Models?

July 11, 2021 Â· 26 min Â· Lilian Weng

Table of Contents

[Updated on 2021â€“09â€“19: Highly recommend this blog post on [score-based generative modeling](https://yang-song.github.io/blog/2021/score/) by Yang Song (author of several key papers in the references)].  
[Updated on 2022â€“08â€“27: Added [classifier-free guidance](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#classifier-free-guidance), [GLIDE](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#glide), [unCLIP](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#unclip) and [Imagen](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#imagen).  
[Updated on 2022â€“08â€“31: Added [latent diffusion model](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#ldm).

So far, Iâ€™ve written about three types of generative models, [GAN](https://lilianweng.github.io/posts/2017-08-20-gan/), [VAE](https://lilianweng.github.io/posts/2018-08-12-vae/), and [Flow-based](https://lilianweng.github.io/posts/2018-10-13-flow-models/) models. They have shown great success in generating high-quality samples, but each has some limitations of its own. GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature. VAE relies on a surrogate loss. Flow models have to use specialized architectures to construct reversible transform.

Diffusion models are inspired by non-equilibrium thermodynamics. They define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise. Unlike VAE or flow models, diffusion models are learned with a fixed procedure and the latent variable has high dimensionality (same as the original data).

![](https://cdn-images-1.medium.com/max/800/0*Ww8kTiIas-65dsOc.png)### What are Diffusion Models?

### 

Several diffusion-based generative models have been proposed with similar ideas underneath, including *diffusion probabilistic models* ([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)), *noise-conditioned score network* (**NCSN**; [Yang & Ermon, 2019](https://arxiv.org/abs/1907.05600)), and *denoising diffusion probabilistic models* (**DDPM**; [Ho et al. 2020](https://arxiv.org/abs/2006.11239)).

### Forward diffusion process

### 

Given a data point sampled from a real data distribution ï¿½0âˆ¼ï¿½(ï¿½), let us define a *forward diffusion process* in which we add small amount of Gaussian noise to the sample in ï¿½ steps, producing a sequence of noisy samples ï¿½1,â€¦,ï¿½ï¿½. The step sizes are controlled by a variance schedule {ï¿½ï¿½âˆˆ(0,1)}ï¿½=1ï¿½.

ï¿½(ï¿½ï¿½|ï¿½ï¿½âˆ’1)=ï¿½(ï¿½ï¿½;1âˆ’ï¿½ï¿½ï¿½ï¿½âˆ’1,ï¿½ï¿½ï¿½)ï¿½(ï¿½1:ï¿½|ï¿½0)=âˆï¿½=1ï¿½ï¿½(ï¿½ï¿½|ï¿½ï¿½âˆ’1)

The data sample ï¿½0 gradually loses its distinguishable features as the step ï¿½ becomes larger. Eventually when ï¿½â†’âˆ, ï¿½ï¿½ is equivalent to an isotropic Gaussian distribution.

![](https://cdn-images-1.medium.com/max/800/0*WMsduHnv9BagImv3.png)A nice property of the above process is that we can sample ï¿½ï¿½ at any arbitrary time step ï¿½ in a closed form using [reparameterization trick](https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick). Let ï¿½ï¿½=1âˆ’ï¿½ï¿½ and ï¿½Â¯ï¿½=âˆï¿½=1ï¿½ï¿½ï¿½:

ï¿½ï¿½=ï¿½ï¿½ï¿½ï¿½âˆ’1+1âˆ’ï¿½ï¿½ï¿½ï¿½âˆ’1Â ;where ï¿½ï¿½âˆ’1,ï¿½ï¿½âˆ’2,â‹¯âˆ¼ï¿½(0,ï¿½)=ï¿½ï¿½ï¿½ï¿½âˆ’1ï¿½ï¿½âˆ’2+1âˆ’ï¿½ï¿½ï¿½ï¿½âˆ’1ï¿½Â¯ï¿½âˆ’2Â ;where ï¿½Â¯ï¿½âˆ’2 merges two Gaussians (\*).=â€¦=ï¿½Â¯ï¿½ï¿½0+1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½(ï¿½ï¿½|ï¿½0)=ï¿½(ï¿½ï¿½;ï¿½Â¯ï¿½ï¿½0,(1âˆ’ï¿½Â¯ï¿½)ï¿½)

(\*) Recall that when we merge two Gaussians with different variance, ï¿½(0,ï¿½12ï¿½) and ï¿½(0,ï¿½22ï¿½), the new distribution is ï¿½(0,(ï¿½12+ï¿½22)ï¿½). Here the merged standard deviation is (1âˆ’ï¿½ï¿½)+ï¿½ï¿½(1âˆ’ï¿½ï¿½âˆ’1)=1âˆ’ï¿½ï¿½ï¿½ï¿½âˆ’1.

Usually, we can afford a larger update step when the sample gets noisier, so ï¿½1<ï¿½2<â‹¯<ï¿½ï¿½ and therefore ï¿½Â¯1>â‹¯>ï¿½Â¯ï¿½.

### Connection with stochastic gradient LangevinÂ dynamics

### 

Langevin dynamics is a concept from physics, developed for statistically modeling molecular systems. Combined with stochastic gradient descent, *stochastic gradient Langevin dynamics* ([Welling & Teh 2011](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)) can produce samples from a probability density ï¿½(ï¿½) using only the gradients âˆ‡ï¿½logâ¡ï¿½(ï¿½) in a Markov chain of updates:

ï¿½ï¿½=ï¿½ï¿½âˆ’1+ï¿½2âˆ‡ï¿½logâ¡ï¿½(ï¿½ï¿½âˆ’1)+ï¿½ï¿½ï¿½,where ï¿½ï¿½âˆ¼ï¿½(0,ï¿½)

where ï¿½ is the step size. When ï¿½â†’âˆ,ï¿½â†’0, ï¿½ï¿½ equals to the true probability density ï¿½(ï¿½).

Compared to standard SGD, stochastic gradient Langevin dynamics injects Gaussian noise into the parameter updates to avoid collapses into local minima.

### Reverse diffusion process

### 

If we can reverse the above process and sample from ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½), we will be able to recreate the true sample from a Gaussian noise input, ï¿½ï¿½âˆ¼ï¿½(0,ï¿½). Note that if ï¿½ï¿½ is small enough, ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½) will also be Gaussian. Unfortunately, we cannot easily estimate ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½) because it needs to use the entire dataset and therefore we need to learn a model ï¿½ï¿½ to approximate these conditional probabilities in order to run the *reverse diffusion process*.

ï¿½ï¿½(ï¿½0:ï¿½)=ï¿½(ï¿½ï¿½)âˆï¿½=1ï¿½ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)=ï¿½(ï¿½ï¿½âˆ’1;ï¿½ï¿½(ï¿½ï¿½,ï¿½),ï¿½ï¿½(ï¿½ï¿½,ï¿½))

![](https://cdn-images-1.medium.com/max/800/0*61YZeGDgwml3U1nR.png)It is noteworthy that the reverse conditional probability is tractable when conditioned on ï¿½0:

ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)=ï¿½(ï¿½ï¿½âˆ’1;ï¿½~(ï¿½ï¿½,ï¿½0),ï¿½~ï¿½ï¿½)

Using Bayesâ€™ rule, we have:

ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)=ï¿½(ï¿½ï¿½|ï¿½ï¿½âˆ’1,ï¿½0)ï¿½(ï¿½ï¿½âˆ’1|ï¿½0)ï¿½(ï¿½ï¿½|ï¿½0)âˆexpâ¡(âˆ’12((ï¿½ï¿½âˆ’ï¿½ï¿½ï¿½ï¿½âˆ’1)2ï¿½ï¿½+(ï¿½ï¿½âˆ’1âˆ’ï¿½Â¯ï¿½âˆ’1ï¿½0)21âˆ’ï¿½Â¯ï¿½âˆ’1âˆ’(ï¿½ï¿½âˆ’ï¿½Â¯ï¿½ï¿½0)21âˆ’ï¿½Â¯ï¿½))=expâ¡(âˆ’12(ï¿½ï¿½2âˆ’2ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½âˆ’1+ï¿½ï¿½ï¿½ï¿½âˆ’12ï¿½ï¿½+ï¿½ï¿½âˆ’12âˆ’2ï¿½Â¯ï¿½âˆ’1ï¿½0ï¿½ï¿½âˆ’1+ï¿½Â¯ï¿½âˆ’1ï¿½021âˆ’ï¿½Â¯ï¿½âˆ’1âˆ’(ï¿½ï¿½âˆ’ï¿½Â¯ï¿½ï¿½0)21âˆ’ï¿½Â¯ï¿½))=expâ¡(âˆ’12((ï¿½ï¿½ï¿½ï¿½+11âˆ’ï¿½Â¯ï¿½âˆ’1)ï¿½ï¿½âˆ’12âˆ’(2ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½+2ï¿½Â¯ï¿½âˆ’11âˆ’ï¿½Â¯ï¿½âˆ’1ï¿½0)ï¿½ï¿½âˆ’1+ï¿½(ï¿½ï¿½,ï¿½0)))

where ï¿½(ï¿½ï¿½,ï¿½0) is some function not involving ï¿½ï¿½âˆ’1 and details are omitted. Following the standard Gaussian density function, the mean and variance can be parameterized as follows (recall that ï¿½ï¿½=1âˆ’ï¿½ï¿½ and ï¿½Â¯ï¿½=âˆï¿½=1ï¿½ï¿½ï¿½):

ï¿½~ï¿½=1/(ï¿½ï¿½ï¿½ï¿½+11âˆ’ï¿½Â¯ï¿½âˆ’1)=1/(ï¿½ï¿½âˆ’ï¿½Â¯ï¿½+ï¿½ï¿½ï¿½ï¿½(1âˆ’ï¿½Â¯ï¿½âˆ’1))=1âˆ’ï¿½Â¯ï¿½âˆ’11âˆ’ï¿½Â¯ï¿½â‹…ï¿½ï¿½ï¿½~ï¿½(ï¿½ï¿½,ï¿½0)=(ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½Â¯ï¿½âˆ’11âˆ’ï¿½Â¯ï¿½âˆ’1ï¿½0)/(ï¿½ï¿½ï¿½ï¿½+11âˆ’ï¿½Â¯ï¿½âˆ’1)=(ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½+ï¿½Â¯ï¿½âˆ’11âˆ’ï¿½Â¯ï¿½âˆ’1ï¿½0)1âˆ’ï¿½Â¯ï¿½âˆ’11âˆ’ï¿½Â¯ï¿½â‹…ï¿½ï¿½=ï¿½ï¿½(1âˆ’ï¿½Â¯ï¿½âˆ’1)1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½+ï¿½Â¯ï¿½âˆ’1ï¿½ï¿½1âˆ’ï¿½Â¯ï¿½ï¿½0

Thanks to the [nice property](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice), we can represent ï¿½0=1ï¿½Â¯ï¿½(ï¿½ï¿½âˆ’1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½) and plug it into the above equation and obtain:

ï¿½~ï¿½=ï¿½ï¿½(1âˆ’ï¿½Â¯ï¿½âˆ’1)1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½+ï¿½Â¯ï¿½âˆ’1ï¿½ï¿½1âˆ’ï¿½Â¯ï¿½1ï¿½Â¯ï¿½(ï¿½ï¿½âˆ’1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½)=1ï¿½ï¿½(ï¿½ï¿½âˆ’1âˆ’ï¿½ï¿½1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½)

As demonstrated in Fig. 2., such a setup is very similar to [VAE](https://lilianweng.github.io/posts/2018-08-12-vae/) and thus we can use the variational lower bound to optimize the negative log-likelihood.

âˆ’logâ¡ï¿½ï¿½(ï¿½0)â‰¤âˆ’logâ¡ï¿½ï¿½(ï¿½0)+ï¿½KL(ï¿½(ï¿½1:ï¿½|ï¿½0)â€–ï¿½ï¿½(ï¿½1:ï¿½|ï¿½0))=âˆ’logâ¡ï¿½ï¿½(ï¿½0)+ï¿½ï¿½1:ï¿½âˆ¼ï¿½(ï¿½1:ï¿½|ï¿½0)[logâ¡ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½(ï¿½0:ï¿½)/ï¿½ï¿½(ï¿½0)]=âˆ’logâ¡ï¿½ï¿½(ï¿½0)+ï¿½ï¿½[logâ¡ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½(ï¿½0:ï¿½)+logâ¡ï¿½ï¿½(ï¿½0)]=ï¿½ï¿½[logâ¡ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½(ï¿½0:ï¿½)]Let ï¿½VLB=ï¿½ï¿½(ï¿½0:ï¿½)[logâ¡ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½(ï¿½0:ï¿½)]â‰¥âˆ’ï¿½ï¿½(ï¿½0)logâ¡ï¿½ï¿½(ï¿½0)

It is also straightforward to get the same result using Jensenâ€™s inequality. Say we want to minimize the cross entropy as the learning objective,

ï¿½CE=âˆ’ï¿½ï¿½(ï¿½0)logâ¡ï¿½ï¿½(ï¿½0)=âˆ’ï¿½ï¿½(ï¿½0)logâ¡(âˆ«ï¿½ï¿½(ï¿½0:ï¿½)ï¿½ï¿½1:ï¿½)=âˆ’ï¿½ï¿½(ï¿½0)logâ¡(âˆ«ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½(ï¿½0:ï¿½)ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½1:ï¿½)=âˆ’ï¿½ï¿½(ï¿½0)logâ¡(ï¿½ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½(ï¿½0:ï¿½)ï¿½(ï¿½1:ï¿½|ï¿½0))â‰¤âˆ’ï¿½ï¿½(ï¿½0:ï¿½)logâ¡ï¿½ï¿½(ï¿½0:ï¿½)ï¿½(ï¿½1:ï¿½|ï¿½0)=ï¿½ï¿½(ï¿½0:ï¿½)[logâ¡ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½(ï¿½0:ï¿½)]=ï¿½VLB

To convert each term in the equation to be analytically computable, the objective can be further rewritten to be a combination of several KL-divergence and entropy terms (See the detailed step-by-step process in Appendix B in [Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)):

ï¿½VLB=ï¿½ï¿½(ï¿½0:ï¿½)[logâ¡ï¿½(ï¿½1:ï¿½|ï¿½0)ï¿½ï¿½(ï¿½0:ï¿½)]=ï¿½ï¿½[logâ¡âˆï¿½=1ï¿½ï¿½(ï¿½ï¿½|ï¿½ï¿½âˆ’1)ï¿½ï¿½(ï¿½ï¿½)âˆï¿½=1ï¿½ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)]=ï¿½ï¿½[âˆ’logâ¡ï¿½ï¿½(ï¿½ï¿½)+âˆ‘ï¿½=1ï¿½logâ¡ï¿½(ï¿½ï¿½|ï¿½ï¿½âˆ’1)ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)]=ï¿½ï¿½[âˆ’logâ¡ï¿½ï¿½(ï¿½ï¿½)+âˆ‘ï¿½=2ï¿½logâ¡ï¿½(ï¿½ï¿½|ï¿½ï¿½âˆ’1)ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)+logâ¡ï¿½(ï¿½1|ï¿½0)ï¿½ï¿½(ï¿½0|ï¿½1)]=ï¿½ï¿½[âˆ’logâ¡ï¿½ï¿½(ï¿½ï¿½)+âˆ‘ï¿½=2ï¿½logâ¡(ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)â‹…ï¿½(ï¿½ï¿½|ï¿½0)ï¿½(ï¿½ï¿½âˆ’1|ï¿½0))+logâ¡ï¿½(ï¿½1|ï¿½0)ï¿½ï¿½(ï¿½0|ï¿½1)]=ï¿½ï¿½[âˆ’logâ¡ï¿½ï¿½(ï¿½ï¿½)+âˆ‘ï¿½=2ï¿½logâ¡ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)+âˆ‘ï¿½=2ï¿½logâ¡ï¿½(ï¿½ï¿½|ï¿½0)ï¿½(ï¿½ï¿½âˆ’1|ï¿½0)+logâ¡ï¿½(ï¿½1|ï¿½0)ï¿½ï¿½(ï¿½0|ï¿½1)]=ï¿½ï¿½[âˆ’logâ¡ï¿½ï¿½(ï¿½ï¿½)+âˆ‘ï¿½=2ï¿½logâ¡ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)+logâ¡ï¿½(ï¿½ï¿½|ï¿½0)ï¿½(ï¿½1|ï¿½0)+logâ¡ï¿½(ï¿½1|ï¿½0)ï¿½ï¿½(ï¿½0|ï¿½1)]=ï¿½ï¿½[logâ¡ï¿½(ï¿½ï¿½|ï¿½0)ï¿½ï¿½(ï¿½ï¿½)+âˆ‘ï¿½=2ï¿½logâ¡ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)âˆ’logâ¡ï¿½ï¿½(ï¿½0|ï¿½1)]=ï¿½ï¿½[ï¿½KL(ï¿½(ï¿½ï¿½|ï¿½0)âˆ¥ï¿½ï¿½(ï¿½ï¿½))âŸï¿½ï¿½+âˆ‘ï¿½=2ï¿½ï¿½KL(ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)âˆ¥ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½))âŸï¿½ï¿½âˆ’1âˆ’logâ¡ï¿½ï¿½(ï¿½0|ï¿½1)âŸï¿½0]

Letâ€™s label each component in the variational lower bound loss separately:

ï¿½VLB=ï¿½ï¿½+ï¿½ï¿½âˆ’1+â‹¯+ï¿½0where ï¿½ï¿½=ï¿½KL(ï¿½(ï¿½ï¿½|ï¿½0)âˆ¥ï¿½ï¿½(ï¿½ï¿½))ï¿½ï¿½=ï¿½KL(ï¿½(ï¿½ï¿½|ï¿½ï¿½+1,ï¿½0)âˆ¥ï¿½ï¿½(ï¿½ï¿½|ï¿½ï¿½+1)) for 1â‰¤ï¿½â‰¤ï¿½âˆ’1ï¿½0=âˆ’logâ¡ï¿½ï¿½(ï¿½0|ï¿½1)

Every KL term in ï¿½VLB (except for ï¿½0) compares two Gaussian distributions and therefore they can be computed in [closed form](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions). ï¿½ï¿½ is constant and can be ignored during training because ï¿½ has no learnable parameters and ï¿½ï¿½ is a Gaussian noise. [Ho et al. 2020](https://arxiv.org/abs/2006.11239) models ï¿½0 using a separate discrete decoder derived from ï¿½(ï¿½0;ï¿½ï¿½(ï¿½1,1),ï¿½ï¿½(ï¿½1,1)).

### Parameterization of ï¿½ï¿½ for TrainingÂ Loss

### 

Recall that we need to learn a neural network to approximate the conditioned probability distributions in the reverse diffusion process, ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½)=ï¿½(ï¿½ï¿½âˆ’1;ï¿½ï¿½(ï¿½ï¿½,ï¿½),ï¿½ï¿½(ï¿½ï¿½,ï¿½)). We would like to train ï¿½ï¿½ to predict ï¿½~ï¿½=1ï¿½ï¿½(ï¿½ï¿½âˆ’1âˆ’ï¿½ï¿½1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½). Because ï¿½ï¿½ is available as input at training time, we can reparameterize the Gaussian noise term instead to make it predict ï¿½ï¿½ from the input ï¿½ï¿½ at time step ï¿½:

ï¿½ï¿½(ï¿½ï¿½,ï¿½)=1ï¿½ï¿½(ï¿½ï¿½âˆ’1âˆ’ï¿½ï¿½1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½(ï¿½ï¿½,ï¿½))Thus ï¿½ï¿½âˆ’1=ï¿½(ï¿½ï¿½âˆ’1;1ï¿½ï¿½(ï¿½ï¿½âˆ’1âˆ’ï¿½ï¿½1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½(ï¿½ï¿½,ï¿½)),ï¿½ï¿½(ï¿½ï¿½,ï¿½))

The loss term ï¿½ï¿½ is parameterized to minimize the difference from ï¿½~Â :

ï¿½ï¿½=ï¿½ï¿½0,ï¿½[12â€–ï¿½ï¿½(ï¿½ï¿½,ï¿½)â€–22â€–ï¿½~ï¿½(ï¿½ï¿½,ï¿½0)âˆ’ï¿½ï¿½(ï¿½ï¿½,ï¿½)â€–2]=ï¿½ï¿½0,ï¿½[12â€–ï¿½ï¿½â€–22â€–1ï¿½ï¿½(ï¿½ï¿½âˆ’1âˆ’ï¿½ï¿½1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½)âˆ’1ï¿½ï¿½(ï¿½ï¿½âˆ’1âˆ’ï¿½ï¿½1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½(ï¿½ï¿½,ï¿½))â€–2]=ï¿½ï¿½0,ï¿½[(1âˆ’ï¿½ï¿½)22ï¿½ï¿½(1âˆ’ï¿½Â¯ï¿½)â€–ï¿½ï¿½â€–22â€–ï¿½ï¿½âˆ’ï¿½ï¿½(ï¿½ï¿½,ï¿½)â€–2]=ï¿½ï¿½0,ï¿½[(1âˆ’ï¿½ï¿½)22ï¿½ï¿½(1âˆ’ï¿½Â¯ï¿½)â€–ï¿½ï¿½â€–22â€–ï¿½ï¿½âˆ’ï¿½ï¿½(ï¿½Â¯ï¿½ï¿½0+1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½,ï¿½)â€–2]

### Simplification

### 

Empirically, [Ho et al. (2020)](https://arxiv.org/abs/2006.11239) found that training the diffusion model works better with a simplified objective that ignores the weighting term:

ï¿½ï¿½simple=ï¿½ï¿½âˆ¼[1,ï¿½],ï¿½0,ï¿½ï¿½[â€–ï¿½ï¿½âˆ’ï¿½ï¿½(ï¿½ï¿½,ï¿½)â€–2]=ï¿½ï¿½âˆ¼[1,ï¿½],ï¿½0,ï¿½ï¿½[â€–ï¿½ï¿½âˆ’ï¿½ï¿½(ï¿½Â¯ï¿½ï¿½0+1âˆ’ï¿½Â¯ï¿½ï¿½ï¿½,ï¿½)â€–2]

The final simple objective is:

ï¿½simple=ï¿½ï¿½simple+ï¿½

where ï¿½ is a constant not depending on ï¿½.

![](https://cdn-images-1.medium.com/max/800/0*aHBlPIYD2OJPFL41.png)### Connection with noise-conditioned score networksÂ (NCSN)

### 

[Song & Ermon (2019)](https://arxiv.org/abs/1907.05600) proposed a score-based generative modeling method where samples are produced via [Langevin dynamics](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#connection-with-stochastic-gradient-langevin-dynamics) using gradients of the data distribution estimated with score matching. The score of each sample ï¿½â€™s density probability is defined as its gradient âˆ‡ï¿½logâ¡ï¿½(ï¿½). A score network ï¿½ï¿½:ï¿½ï¿½â†’ï¿½ï¿½ is trained to estimate it, ï¿½ï¿½(ï¿½)â‰ˆâˆ‡ï¿½logâ¡ï¿½(ï¿½).

To make it scalable with high-dimensional data in the deep learning setting, they proposed to use either *denoising score matching* ([Vincent, 2011](http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf)) or *sliced score matching* (use random projections; [Song et al., 2019](https://arxiv.org/abs/1905.07088)). Denosing score matching adds a pre-specified small noise to the data ï¿½(ï¿½~|ï¿½) and estimates ï¿½(ï¿½~) with score matching.

Recall that Langevin dynamics can sample data points from a probability density distribution using only the score âˆ‡ï¿½logâ¡ï¿½(ï¿½) in an iterative process.

However, according to the manifold hypothesis, most of the data is expected to concentrate in a low dimensional manifold, even though the observed data might look only arbitrarily high-dimensional. It brings a negative effect on score estimation since the data points cannot cover the whole space. In regions where data density is low, the score estimation is less reliable. After adding a small Gaussian noise to make the perturbed data distribution cover the full space ï¿½ï¿½, the training of the score estimator network becomes more stable. [Song & Ermon (2019)](https://arxiv.org/abs/1907.05600) improved it by perturbing the data with the noise of *different levels* and train a noise-conditioned score network to *jointly* estimate the scores of all the perturbed data at different noise levels.

The schedule of increasing noise levels resembles the forward diffusion process. If we use the diffusion process annotation, the score approximates ï¿½ï¿½(ï¿½ï¿½,ï¿½)â‰ˆâˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½ï¿½). Given a Gaussian distribution ï¿½âˆ¼ï¿½(ï¿½,ï¿½2ï¿½), we can write the derivative of the logarithm of its density function as âˆ‡ï¿½logâ¡ï¿½(ï¿½)=âˆ‡ï¿½(âˆ’12ï¿½2(ï¿½âˆ’ï¿½)2)=âˆ’ï¿½âˆ’ï¿½ï¿½2=âˆ’ï¿½ï¿½ where ï¿½âˆ¼ï¿½(0,ï¿½). [Recall](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice) that ï¿½(ï¿½ï¿½|ï¿½0)âˆ¼ï¿½(ï¿½Â¯ï¿½ï¿½0,(1âˆ’ï¿½Â¯ï¿½)ï¿½) and therefore,

ï¿½ï¿½(ï¿½ï¿½,ï¿½)â‰ˆâˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½ï¿½)=ï¿½ï¿½(ï¿½0)[âˆ‡ï¿½ï¿½ï¿½(ï¿½ï¿½|ï¿½0)]=ï¿½ï¿½(ï¿½0)[âˆ’ï¿½ï¿½(ï¿½ï¿½,ï¿½)1âˆ’ï¿½Â¯ï¿½]=âˆ’ï¿½ï¿½(ï¿½ï¿½,ï¿½)1âˆ’ï¿½Â¯ï¿½

### Parameterization ofÂ ï¿½ï¿½

### 

The forward variances are set to be a sequence of linearly increasing constants in [Ho et al. (2020)](https://arxiv.org/abs/2006.11239), from ï¿½1=10âˆ’4 to ï¿½ï¿½=0.02. They are relatively small compared to the normalized image pixel values between [âˆ’1,1]. Diffusion models in their experiments showed high-quality samples but still could not achieve competitive model log-likelihood as other generative models.

[Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) proposed several improvement techniques to help diffusion models to obtain lower NLL. One of the improvements is to use a cosine-based variance schedule. The choice of the scheduling function can be arbitrary, as long as it provides a near-linear drop in the middle of the training process and subtle changes around ï¿½=0 and ï¿½=ï¿½.

ï¿½ï¿½=clip(1âˆ’ï¿½Â¯ï¿½ï¿½Â¯ï¿½âˆ’1,0.999)ï¿½Â¯ï¿½=ï¿½(ï¿½)ï¿½(0)where ï¿½(ï¿½)=cosâ¡(ï¿½/ï¿½+ï¿½1+ï¿½â‹…ï¿½2)2

where the small offset ï¿½ is to prevent ï¿½ï¿½ from being too small when close to ï¿½=0.

![](https://cdn-images-1.medium.com/max/800/0*MavQcFyBGx2J2v40.png)### Parameterization of reverse process varianceÂ ï¿½ï¿½

### 

[Ho et al. (2020)](https://arxiv.org/abs/2006.11239) chose to fix ï¿½ï¿½ as constants instead of making them learnable and set ï¿½ï¿½(ï¿½ï¿½,ï¿½)=ï¿½ï¿½2ï¿½Â , where ï¿½ï¿½ is not learned but set to ï¿½ï¿½ or ï¿½~ï¿½=1âˆ’ï¿½Â¯ï¿½âˆ’11âˆ’ï¿½Â¯ï¿½â‹…ï¿½ï¿½. Because they found that learning a diagonal variance ï¿½ï¿½ leads to unstable training and poorer sample quality.

[Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) proposed to learn ï¿½ï¿½(ï¿½ï¿½,ï¿½) as an interpolation between ï¿½ï¿½ and ï¿½~ï¿½ by model predicting a mixing vector ï¿½Â :

ï¿½ï¿½(ï¿½ï¿½,ï¿½)=expâ¡(ï¿½logâ¡ï¿½ï¿½+(1âˆ’ï¿½)logâ¡ï¿½~ï¿½)

However, the simple objective ï¿½simple does not depend on ï¿½ï¿½Â . To add the dependency, they constructed a hybrid objective ï¿½hybrid=ï¿½simple+ï¿½ï¿½VLB where ï¿½=0.001 is small and stop gradient on ï¿½ï¿½ in the ï¿½VLB term such that ï¿½VLB only guides the learning of ï¿½ï¿½. Empirically they observed that ï¿½VLB is pretty challenging to optimize likely due to noisy gradients, so they proposed to use a time-averaging smoothed version of ï¿½VLB with importance sampling.

![](https://cdn-images-1.medium.com/max/800/0*s3KcCXGNC5qPfBZk.png)### Speed up Diffusion ModelÂ Sampling

### 

It is very slow to generate a sample from DDPM by following the Markov chain of the reverse diffusion process, as ï¿½ can be up to one or a few thousand steps. One data point from [Song et al. 2020](https://arxiv.org/abs/2010.02502): â€œFor example, it takes around 20 hours to sample 50k images of size 32 Ã— 32 from a DDPM, but less than a minute to do so from a GAN on an Nvidia 2080 Ti GPU.â€

One simple way is to run a strided sampling schedule ([Nichol & Dhariwal, 2021](https://arxiv.org/abs/2102.09672)) by taking the sampling update every âŒˆï¿½/ï¿½âŒ‰ steps to reduce the process from ï¿½ to ï¿½ steps. The new sampling schedule for generation is {ï¿½1,â€¦,ï¿½ï¿½} where ï¿½1<ï¿½2<â‹¯<ï¿½ï¿½âˆˆ[1,ï¿½] and ï¿½<ï¿½.

For another approach, letâ€™s rewrite ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0) to be parameterized by a desired standard deviation ï¿½ï¿½ according to the [nice property](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice):

ï¿½ï¿½âˆ’1=ï¿½Â¯ï¿½âˆ’1ï¿½0+1âˆ’ï¿½Â¯ï¿½âˆ’1ï¿½ï¿½âˆ’1=ï¿½Â¯ï¿½âˆ’1ï¿½0+1âˆ’ï¿½Â¯ï¿½âˆ’1âˆ’ï¿½ï¿½2ï¿½ï¿½+ï¿½ï¿½ï¿½=ï¿½Â¯ï¿½âˆ’1ï¿½0+1âˆ’ï¿½Â¯ï¿½âˆ’1âˆ’ï¿½ï¿½2ï¿½ï¿½âˆ’ï¿½Â¯ï¿½ï¿½01âˆ’ï¿½Â¯ï¿½+ï¿½ï¿½ï¿½ï¿½ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)=ï¿½(ï¿½ï¿½âˆ’1;ï¿½Â¯ï¿½âˆ’1ï¿½0+1âˆ’ï¿½Â¯ï¿½âˆ’1âˆ’ï¿½ï¿½2ï¿½ï¿½âˆ’ï¿½Â¯ï¿½ï¿½01âˆ’ï¿½Â¯ï¿½,ï¿½ï¿½2ï¿½)

Recall that in ï¿½(ï¿½ï¿½âˆ’1|ï¿½ï¿½,ï¿½0)=ï¿½(ï¿½ï¿½âˆ’1;ï¿½~(ï¿½ï¿½,ï¿½0),ï¿½~ï¿½ï¿½), therefore we have:

ï¿½~ï¿½=ï¿½ï¿½2=1âˆ’ï¿½Â¯ï¿½âˆ’11âˆ’ï¿½Â¯ï¿½â‹…ï¿½ï¿½

Let ï¿½ï¿½2=ï¿½â‹…ï¿½~ï¿½ such that we can adjust ï¿½âˆˆï¿½+ as a hyperparameter to control the sampling stochasticity. The special case of ï¿½=0 makes the sampling process *deterministic*. Such a model is named the *denoising diffusion implicit model* (**DDIM**; [Song et al., 2020](https://arxiv.org/abs/2010.02502)). DDIM has the same marginal noise distribution but deterministically maps noise back to the original data samples.

During generation, we only sample a subset of ï¿½ diffusion steps {ï¿½1,â€¦,ï¿½ï¿½} and the inference process becomes:

ï¿½ï¿½,ï¿½(ï¿½ï¿½ï¿½âˆ’1|ï¿½ï¿½ï¿½,ï¿½0)=ï¿½(ï¿½ï¿½ï¿½âˆ’1;ï¿½Â¯ï¿½âˆ’1ï¿½0+1âˆ’ï¿½Â¯ï¿½âˆ’1âˆ’ï¿½ï¿½2ï¿½ï¿½ï¿½âˆ’ï¿½Â¯ï¿½ï¿½01âˆ’ï¿½Â¯ï¿½,ï¿½ï¿½2ï¿½)

While all the models are trained with ï¿½=1000 diffusion steps in the experiments, they observed that DDIM (ï¿½=0) can produce the best quality samples when ï¿½ is small, while DDPM (ï¿½=1) performs much worse on small ï¿½. DDPM does perform better when we can afford to run the full reverse Markov diffusion steps (ï¿½=ï¿½=1000). With DDIM, it is possible to train the diffusion model up to any arbitrary number of forward steps but only sample from a subset of steps in the generative process.

![](https://cdn-images-1.medium.com/max/800/0*riGclOZ7j9SstxXb.png)Compared to DDPM, DDIM is able to:

1. Generate higher-quality samples using a much fewer number of steps.
2. Have â€œconsistencyâ€ property since the generative process is deterministic, meaning that multiple samples conditioned on the same latent variable should have similar high-level features.
3. Because of the consistency, DDIM can do semantically meaningful interpolation in the latent variable.

*Latent diffusion model* (**LDM**; [Rombach & Blattmann, et al. 2022](https://arxiv.org/abs/2112.10752)) runs the diffusion process in the latent space instead of pixel space, making training cost lower and inference speed faster. It is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression. LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.

![](https://cdn-images-1.medium.com/max/800/0*2cJD5Za96QdQJNQJ.png)The perceptual compression process relies on an autoencoder model. An encoder ï¿½ is used to compress the input image ï¿½âˆˆï¿½ï¿½Ã—ï¿½Ã—3 to a smaller 2D latent vector ï¿½=ï¿½(ï¿½)âˆˆï¿½â„Ã—ï¿½Ã—ï¿½Â , where the downsampling rate ï¿½=ï¿½/â„=ï¿½/ï¿½=2ï¿½,ï¿½âˆˆï¿½. Then an decoder ï¿½ reconstructs the images from the latent vector, ï¿½~=ï¿½(ï¿½). The paper explored two types of regularization in autoencoder training to avoid arbitrarily high-variance in the latent spaces.

* KL-reg: A small KL penalty towards a standard normal distribution over the learned latent, similar to [VAE](https://lilianweng.github.io/posts/2018-08-12-vae/).
* VQ-reg: Uses a vector quantization layer within the decoder, like [VQVAE](https://lilianweng.github.io/posts/2018-08-12-vae/#vq-vae-and-vq-vae-2) but the quantization layer is absorbed by the decoder.

The diffusion and denoising processes happen on the latent vector ï¿½. The denoising model is a time-conditioned U-Net, augmented with the cross-attention mechanism to handle flexible conditioning information for image generation (e.g. class labels, semantic maps, blurred variants of an image). The design is equivalent to fuse representation of different modality into the model with cross-attention mechanism. Each type of conditioning information is paired with a domain-specific encoder ï¿½ï¿½ to project the conditioning input ï¿½ to an intermediate representation that can be mapped into cross-attention component, ï¿½ï¿½(ï¿½)âˆˆï¿½ï¿½Ã—ï¿½ï¿½:

Attention(ï¿½,ï¿½,ï¿½)=softmax(ï¿½ï¿½âŠ¤ï¿½)â‹…ï¿½where ï¿½=ï¿½ï¿½(ï¿½)â‹…ï¿½ï¿½(ï¿½ï¿½),ï¿½=ï¿½ï¿½(ï¿½)â‹…ï¿½ï¿½(ï¿½),ï¿½=ï¿½ï¿½(ï¿½)â‹…ï¿½ï¿½(ï¿½)and ï¿½ï¿½(ï¿½)âˆˆï¿½ï¿½Ã—ï¿½ï¿½ï¿½,ï¿½ï¿½(ï¿½),ï¿½ï¿½(ï¿½)âˆˆï¿½ï¿½Ã—ï¿½ï¿½,ï¿½ï¿½(ï¿½ï¿½)âˆˆï¿½ï¿½Ã—ï¿½ï¿½ï¿½,ï¿½ï¿½(ï¿½)âˆˆï¿½ï¿½Ã—ï¿½ï¿½

![](https://cdn-images-1.medium.com/max/800/0*ia-MjXlmIZ6INZxS.png)### Conditioned Generation

### 

While training generative models on images with conditioning information such as ImageNet dataset, it is common to generate samples conditioned on class labels or a piece of descriptive text.

### Classifier Guided Diffusion

### 

To explicit incorporate class information into the diffusion process, [Dhariwal & Nichol (2021)](https://arxiv.org/abs/2105.05233) trained a classifier ï¿½ï¿½(ï¿½|ï¿½ï¿½,ï¿½) on noisy image ï¿½ï¿½ and use gradients âˆ‡ï¿½logâ¡ï¿½ï¿½(ï¿½|ï¿½ï¿½) to guide the diffusion sampling process toward the conditioning information ï¿½ (e.g. a target class label) by altering the noise prediction. [Recall](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#score) that âˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½ï¿½)=âˆ’11âˆ’ï¿½Â¯ï¿½ï¿½ï¿½(ï¿½ï¿½,ï¿½) and we can write the score function for the joint distribution ï¿½(ï¿½ï¿½,ï¿½) as following,

âˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½ï¿½,ï¿½)=âˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½ï¿½)+âˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½|ï¿½ï¿½)â‰ˆâˆ’11âˆ’ï¿½Â¯ï¿½ï¿½ï¿½(ï¿½ï¿½,ï¿½)+âˆ‡ï¿½ï¿½logâ¡ï¿½ï¿½(ï¿½|ï¿½ï¿½)=âˆ’11âˆ’ï¿½Â¯ï¿½(ï¿½ï¿½(ï¿½ï¿½,ï¿½)âˆ’1âˆ’ï¿½Â¯ï¿½âˆ‡ï¿½ï¿½logâ¡ï¿½ï¿½(ï¿½|ï¿½ï¿½))

Thus, a new classifier-guided predictor ï¿½Â¯ï¿½ would take the form as following,

ï¿½Â¯ï¿½(ï¿½ï¿½,ï¿½)=ï¿½ï¿½(ï¿½ï¿½,ï¿½)âˆ’1âˆ’ï¿½Â¯ï¿½âˆ‡ï¿½ï¿½logâ¡ï¿½ï¿½(ï¿½|ï¿½ï¿½)

To control the strength of the classifier guidance, we can add a weight ï¿½ to the delta part,

ï¿½Â¯ï¿½(ï¿½ï¿½,ï¿½)=ï¿½ï¿½(ï¿½ï¿½,ï¿½)âˆ’1âˆ’ï¿½Â¯ï¿½ï¿½âˆ‡ï¿½ï¿½logâ¡ï¿½ï¿½(ï¿½|ï¿½ï¿½)

The resulting *ablated diffusion model* (**ADM**) and the one with additional classifier guidance (**ADM-G**) are able to achieve better results than SOTA generative models (e.g. BigGAN).

![](https://cdn-images-1.medium.com/max/800/0*bxKRprZyS4mwV2Sa.png)Additionally with some modifications on the U-Net architecture, [Dhariwal & Nichol (2021)](https://arxiv.org/abs/2105.05233) showed performance better than GAN with diffusion models. The architecture modifications include larger model depth/width, more attention heads, multi-resolution attention, BigGAN residual blocks for up/downsampling, residual connection rescale by 1/2 and adaptive group normalization (AdaGN).

### Classifier-Free Guidance

### 

Without an independent classifier ï¿½ï¿½, it is still possible to run conditional diffusion steps by incorporating the scores from a conditional and an unconditional diffusion model ([Ho & Salimans, 2021](https://openreview.net/forum?id=qw8AKxfYbI)). Let unconditional denoising diffusion model ï¿½ï¿½(ï¿½) parameterized through a score estimator ï¿½ï¿½(ï¿½ï¿½,ï¿½) and the conditional model ï¿½ï¿½(ï¿½|ï¿½) parameterized through ï¿½ï¿½(ï¿½ï¿½,ï¿½,ï¿½). These two models can be learned via a single neural network. Precisely, a conditional diffusion model ï¿½ï¿½(ï¿½|ï¿½) is trained on paired data (ï¿½,ï¿½), where the conditioning information ï¿½ gets discarded periodically at random such that the model knows how to generate images unconditionally as well, i.e. ï¿½ï¿½(ï¿½ï¿½,ï¿½)=ï¿½ï¿½(ï¿½ï¿½,ï¿½,ï¿½=âˆ…).

The gradient of an implicit classifier can be represented with conditional and unconditional score estimators. Once plugged into the classifier-guided modified score, the score contains no dependency on a separate classifier.

âˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½|ï¿½ï¿½)=âˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½ï¿½|ï¿½)âˆ’âˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½ï¿½)=âˆ’11âˆ’ï¿½Â¯ï¿½(ï¿½ï¿½(ï¿½ï¿½,ï¿½,ï¿½)âˆ’ï¿½ï¿½(ï¿½ï¿½,ï¿½))ï¿½Â¯ï¿½(ï¿½ï¿½,ï¿½,ï¿½)=ï¿½ï¿½(ï¿½ï¿½,ï¿½,ï¿½)âˆ’1âˆ’ï¿½Â¯ï¿½ï¿½âˆ‡ï¿½ï¿½logâ¡ï¿½(ï¿½|ï¿½ï¿½)=ï¿½ï¿½(ï¿½ï¿½,ï¿½,ï¿½)+ï¿½(ï¿½ï¿½(ï¿½ï¿½,ï¿½,ï¿½)âˆ’ï¿½ï¿½(ï¿½ï¿½,ï¿½))=(ï¿½+1)ï¿½ï¿½(ï¿½ï¿½,ï¿½,ï¿½)âˆ’ï¿½ï¿½ï¿½(ï¿½ï¿½,ï¿½)

Their experiments showed that classifier-free guidance can achieve a good balance between FID (distinguish between synthetic and generated images) and IS (quality and diversity).

The guided diffusion model, GLIDE ([Nichol, Dhariwal & Ramesh, et al. 2022](https://arxiv.org/abs/2112.10741)), explored both guiding strategies, CLIP guidance and classifier-free guidance, and found that the latter is more preferred. They hypothesized that it is because CLIP guidance exploits the model with adversarial examples towards the CLIP model, rather than optimize the better matched images generation.

### Scale up Generation Resolution andÂ Quality

### 

To generate high-quality images at high resolution, [Ho et al. (2021)](https://arxiv.org/abs/2106.15282) proposed to use a pipeline of multiple diffusion models at increasing resolutions. *Noise conditioning augmentation* between pipeline models is crucial to the final image quality, which is to apply strong data augmentation to the conditioning input ï¿½ of each super-resolution model ï¿½ï¿½(ï¿½|ï¿½). The conditioning noise helps reduce compounding error in the pipeline setup. *U-net* is a common choice of model architecture in diffusion modeling for high-resolution image generation.

![](https://cdn-images-1.medium.com/max/800/0*RKULSx8V9b5w9NOO.png)They found the most effective noise is to apply Gaussian noise at low resolution and Gaussian blur at high resolution. In addition, they also explored two forms of conditioning augmentation that require small modification to the training process. Note that conditioning noise is only applied to training but not at inference.

* Truncated conditioning augmentation stops the diffusion process early at step ï¿½>0 for low resolution.
* Non-truncated conditioning augmentation runs the full low resolution reverse process until step 0 but then corrupt it by ï¿½ï¿½âˆ¼ï¿½(ï¿½ï¿½|ï¿½0) and then feeds the corrupted ï¿½ï¿½ s into the super-resolution model.

The two-stage diffusion model **unCLIP** ([Ramesh et al. 2022](https://arxiv.org/abs/2204.06125)) heavily utilizes the CLIP text encoder to produce text-guided images at high quality. Given a pretrained CLIP model ï¿½ and paired training data for the diffusion model, (ï¿½,ï¿½), where ï¿½ is an image and ï¿½ is the corresponding caption, we can compute the CLIP text and image embedding, ï¿½ï¿½(ï¿½) and ï¿½ï¿½(ï¿½), respectively. The unCLIP learns two models in parallel:

* A prior model ï¿½(ï¿½ï¿½|ï¿½): outputs CLIP image embedding ï¿½ï¿½ given the text ï¿½.
* A decoder ï¿½(ï¿½|ï¿½ï¿½,[ï¿½]): generates the image ï¿½ given CLIP image embedding ï¿½ï¿½ and optionally the original text ï¿½.

These two models enable conditional generation, because

ï¿½(ï¿½|ï¿½)=ï¿½(ï¿½,ï¿½ï¿½|ï¿½)âŸï¿½ï¿½ is deterministic given ï¿½=ï¿½(ï¿½|ï¿½ï¿½,ï¿½)ï¿½(ï¿½ï¿½|ï¿½)

![](https://cdn-images-1.medium.com/max/800/0*i-dKwV3wi9mUbksw.png)unCLIP follows a two-stage image generation process:

1. Given a text ï¿½, a CLIP model is first used to generate a text embedding ï¿½ï¿½(ï¿½). Using CLIP latent space enables zero-shot image manipulation via text.
2. A diffusion or autoregressive prior ï¿½(ï¿½ï¿½|ï¿½) processes this CLIP text embedding to construct an image prior and then a diffusion decoder ï¿½(ï¿½|ï¿½ï¿½,[ï¿½]) generates an image, conditioned on the prior. This decoder can also generate image variations conditioned on an image input, preserving its style and semantics.

Instead of CLIP model, **Imagen** ([Saharia et al. 2022](https://arxiv.org/abs/2205.11487)) uses a pre-trained large LM (i.e. a frozen T5-XXL text encoder) to encode text for image generation. There is a general trend that larger model size can lead to better image quality and text-image alignment. They found that T5-XXL and CLIP text encoder achieve similar performance on MS-COCO, but human evaluation prefers T5-XXL on DrawBench (a collection of prompts covering 11 categories).

When applying classifier-free guidance, increasing ï¿½ may lead to better image-text alignment but worse image fidelity. They found that it is due to train-test mismatch, that is saying, because training data ï¿½ stays within the range [âˆ’1,1], the test data should be so too. Two thresholding strategies are introduced:

* Static thresholding: clip ï¿½ prediction to [âˆ’1,1]
* Dynamic thresholding: at each sampling step, compute ï¿½ as a certain percentile absolute pixel value; if ï¿½>1, clip the prediction to [âˆ’ï¿½,ï¿½] and divide by ï¿½.

Imagen modifies several designs in U-net to make it *efficient U-Net*.

* Shift model parameters from high resolution blocks to low resolution by adding more residual locks for the lower resolutions;
* Scale the skip connections by 1/2
* Reverse the order of downsampling (move it before convolutions) and upsampling operations (move it after convolution) in order to improve the speed of forward pass.

They found that noise conditioning augmentation, dynamic thresholding and efficient U-Net are critical for image quality, but scaling text encoder size is more important than U-Net size.

### Quick Summary

### 

* **Pros**: Tractability and flexibility are two conflicting objectives in generative modeling. Tractable models can be analytically evaluated and cheaply fit data (e.g. via a Gaussian or Laplace), but they cannot easily describe the structure in rich datasets. Flexible models can fit arbitrary structures in data, but evaluating, training, or sampling from these models is usually expensive. Diffusion models are both analytically tractable and flexible
* 
* **Cons**: Diffusion models rely on a long Markov chain of diffusion steps to generate samples, so it can be quite expensive in terms of time and compute. New methods have been proposed to make the process much faster, but the sampling is still slower than GAN.
* 

### Citation

### 

Cited as:


> *Weng, Lilian. (Jul 2021). What are diffusion models? Lilâ€™Log.* [*https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.*](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.)


>   
> 


>   
> 

Or


```
@article{weng2021diffusion,  

```

```
  

```
### References

### 

[1] Jascha Sohl-Dickstein et al. [â€œDeep Unsupervised Learning using Nonequilibrium Thermodynamics.â€](https://arxiv.org/abs/1503.03585) ICML 2015.

[2] Max Welling & Yee Whye Teh. [â€œBayesian learning via stochastic gradient langevin dynamics.â€](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf) ICML 2011.

[3] Yang Song & Stefano Ermon. [â€œGenerative modeling by estimating gradients of the data distribution.â€](https://arxiv.org/abs/1907.05600) NeurIPS 2019.

[4] Yang Song & Stefano Ermon. [â€œImproved techniques for training score-based generative models.â€](https://arxiv.org/abs/2006.09011) NeuriPS 2020.

[5] Jonathan Ho et al. [â€œDenoising diffusion probabilistic models.â€](https://arxiv.org/abs/2006.11239) arxiv Preprint arxiv:2006.11239 (2020). [[code](https://github.com/hojonathanho/diffusion)]

[6] Jiaming Song et al. [â€œDenoising diffusion implicit models.â€](https://arxiv.org/abs/2010.02502) arxiv Preprint arxiv:2010.02502 (2020). [[code](https://github.com/ermongroup/ddim)]

[7] Alex Nichol & Prafulla Dhariwal. [â€œImproved denoising diffusion probabilistic modelsâ€](https://arxiv.org/abs/2102.09672) arxiv Preprint arxiv:2102.09672 (2021). [[code](https://github.com/openai/improved-diffusion)]

[8] Prafula Dhariwal & Alex Nichol. [â€œDiffusion Models Beat GANs on Image Synthesis.â€](https://arxiv.org/abs/2105.05233) arxiv Preprint arxiv:2105.05233 (2021). [[code](https://github.com/openai/guided-diffusion)]

[9] Jonathan Ho & Tim Salimans. [â€œClassifier-Free Diffusion Guidance.â€](https://arxiv.org/abs/2207.12598) NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications.

[10] Yang Song, et al. [â€œScore-Based Generative Modeling through Stochastic Differential Equations.â€](https://openreview.net/forum?id=PxTIG12RRHS) ICLR 2021.

[11] Alex Nichol, Prafulla Dhariwal & Aditya Ramesh, et al. [â€œGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.â€](https://arxiv.org/abs/2112.10741) ICML 2022.

[12] Jonathan Ho, et al. [â€œCascaded diffusion models for high fidelity image generation.â€](https://arxiv.org/abs/2106.15282) J. Mach. Learn. Res. 23 (2022): 47â€“1.

[13] Aditya Ramesh et al. [â€œHierarchical Text-Conditional Image Generation with CLIP Latents.â€](https://arxiv.org/abs/2204.06125) arxiv Preprint arxiv:2204.06125 (2022).

[14] Chitwan Saharia & William Chan, et al. [â€œPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding.â€](https://arxiv.org/abs/2205.11487) arxiv Preprint arxiv:2205.11487 (2022).

[15] Rombach & Blattmann, et al. [â€œHigh-Resolution Image Synthesis with Latent Diffusion Models.â€](https://arxiv.org/abs/2112.10752) CVPR 2022.[code](https://github.com/CompVis/latent-diffusion)

* [generative-model](https://lilianweng.github.io/tags/generative-model/)
* [math-heavy](https://lilianweng.github.io/tags/math-heavy/)
* [image-generation](https://lilianweng.github.io/tags/image-generation/)

[Â«  
How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/posts/2021-09-25-train-large/)[Â»  
Contrastive Representation Learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)

Â© 2023 [Lilâ€™Log](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/) & [PaperMod](https://git.io/hugopapermod)

  


Also, I will be going to the [LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/) where I hope to learn more best practices on productionize LLMs-powered apps. If you are interested in this topic, stay tuned for my future posts! ğŸ˜ƒ



[View original.](https://medium.com/p/c53da8a2d4c5)

Exported from [Medium](https://medium.com) on May 25, 2024.

