---
title: 'Sion Models For Image Generation'
date: draft_Diff
permalink: /posts/draft_D/Diffusion-Models/
tags:
  - cool posts
  - category1
  - category2
---

Diffusion Models for Image Generation
 \* {
 font-family: Georgia, Cambria, "Times New Roman", Times, serif;
 }
 html, body {
 margin: 0;
 padding: 0;
 }
 h1 {
 font-size: 50px;
 margin-bottom: 17px;
 color: #333;
 }
 h2 {
 font-size: 24px;
 line-height: 1.6;
 margin: 30px 0 0 0;
 margin-bottom: 18px;
 margin-top: 33px;
 color: #333;
 }
 h3 {
 font-size: 30px;
 margin: 10px 0 20px 0;
 color: #333;
 }
 header {
 width: 640px;
 margin: auto;
 }
 section {
 width: 640px;
 margin: auto;
 }
 section p {
 margin-bottom: 27px;
 font-size: 20px;
 line-height: 1.6;
 color: #333;
 }
 section img {
 max-width: 640px;
 }
 footer {
 padding: 0 20px;
 margin: 50px 0;
 text-align: center;
 font-size: 12px;
 }
 .aspectRatioPlaceholder {
 max-width: auto !important;
 max-height: auto !important;
 }
 .aspectRatioPlaceholder-fill {
 padding-bottom: 0 !important;
 }
 header,
 section[data-field=subtitle],
 section[data-field=description] {
 display: none;
 }
 

Diffusion Models for Image Generation
=====================================




Generative AI models aim to generate realistic and diverse data samples. While many successful generative models suffer from instability‚Ä¶




---

### Diffusion Models for Image Generation

Generative AI models aim to generate realistic and diverse data samples. While many successful generative models suffer from instability during training and mode collapse, where the model generates limited and repetitive samples. The Stable Diffusion Model (SDM) is a generative model that addresses these issues by incorporating a diffusion process. Here we introduce the overall Diffusion Model, how it works, the differential with other models, why it addresses the above issues, and its advantages over the before the era of the stable diffusion generative models.

### Overview of the Stable Diffusion Model

The SDM is a generative model that employs a diffusion process to generate samples. The diffusion process involves iteratively applying noise to the data, making it more and more diffuse. This process creates a continuum of intermediate states that are used to generate samples. The SDM is trained to invert this diffusion process to generate samples from the intermediate states.

The SDM is a stochastic model, which means that it generates samples by drawing random samples from a probability distribution. The probability distribution is defined by a series of Gaussian distributions that are parameterized by a neural network. The neural network learns to adjust the parameters of the Gaussian distributions to generate samples that match the input data.

#### Unique Properties of the Stable Diffusion Model

The SDM has unique properties that distinguish it from other generative models:

* The SDM generates samples through a diffusion process, which makes it more stable during training. The diffusion process adds noise to the data, making it more robust to overfitting and less prone to mode collapse.
* The SDM generates samples from a continuum of intermediate states rather than discrete samples. This makes it easier to generate diverse samples that are not limited to a specific set of modes.
* The SDM can be controlled by adjusting the diffusion process. By controlling the rate of diffusion, the SDM can generate samples that are more or less similar to the input data. This allows users to generate more or fewer samples similar to the input data, depending on their needs.

### Advantages of the Stable Diffusion Model

Let‚Äôs see what the core advantages SDM has over existing generative models are:

1. The SDM is more stable during training, making it less prone to mode collapse. This means the SDM can generate more diverse samples and avoid generating repetitive ones.
2. The SDM generates samples from a continuum of intermediate states, making it easier to generate diverse samples not limited to a specific set of modes. This allows users to generate more diverse and interesting samples that are not limited to a specific set of modes.
3. The SDM can be controlled by adjusting the diffusion process, which makes it more flexible and adaptable to different use cases. This allows users to generate more or fewer samples similar to the input data, depending on their needs.

Examples

To illustrate the capabilities of the SDM, we will provide examples of its output. The first example is a set of images of faces generated by the SDM. The SDM was trained on a dataset of faces and was able to generate realistic and diverse samples that captured the variability of the input data.

The second example is a set of images of flowers generated by the SDM. The SDM was trained on a flower dataset and could generate samples that capture the shape and color of the input data.

### Conclusion

The Stable Diffusion Model is a powerful generative model that incorporates a diffusion process to generate samples. The diffusion process makes the SDM more stable during training and less prone to mode collapse. The SDM generates samples from a continuum of intermediate states, making generating diverse and interesting samples easier. Finally, the SDM can be controlled by adjusting the diffusion process, making it more flexible and adaptable to different use cases.

  


Generative modeling defines how a dataset is generated. It tries to understand the distribution of data points, providing a model of how the data is generated using a probabilistic model. (e.g., support vector machines or the perceptron algorithm gives a separating decision boundary, but no model for generating synthetic data points). The aim is to generate new samples from what has already been distributed in the training data.

Assume you have an autonomous driving dataset with an urban-scene setting. You now want to generate images from it that are semantically and spatially similar. This is a perfect example of a generative modeling problem. To do this, the generative model must understand the data‚Äôs underlying structure and learn the realistic, generalized representation of the dataset, such as the sky is blue, buildings are usually tall, and pedestrians use sidewalks.

Diffusion models in deep learning were first introduced by S[oul-Dickstein et al. in the original 2015 paper ‚ÄúDeep Unsupervised Learning Using Non-Equilibrium Thermodynamics‚Äù](https://arxiv.org/abs/1503.03585). Unfortunately, this remained behind the scenes for a while.

But in 2019, [Song et al. published a paper called Generative Modeling by Estimating Data Distribution Gradients](https://arxiv.org/abs/1907.05600) using the same principle but a different approach. In 2020, [Ho et al. published the currently popular paper ‚ÄúProbabilistic Noise Diffusion Models‚Äù](https://arxiv.org/abs/2006.11239) (abbreviated as DDPM).

After 2020, research on diffusion models has begun üöÄ. In a relatively short time, significant progress has been made in building, training, and improving diffusion-based generative modeling.

  


Direct Distribution:

The original image (x 0) is slowly iteratively distorted (Markov chain) by adding (scaled Gaussian) noise.

This process is performed for some time steps T, i.e. xT.

Image at time step t is created: xt-1 + Œµt-1 (noise) ‚Üí xt

At this stage, the model is not involved.

At the end of the direct diffusion step Xt, due to the iterative addition of noise, we are left with a (clean) noisy image representing an ‚Äúisotropic Gaussian‚Äù. It‚Äôs just a mathematical way of saying that we have a standard normal distribution and the variance of the distribution is the same across all dimensions. We have converted the distribution of the data to a Gaussian distribution.

Reverse / Reverse Diffusion:

At this point, we cancel the forwarding process. The challenge is to remove the noise added in the direct process again in an iterative way (Markov chain). This is done using a neural network model.

The task of the model is as follows: Given a time interval t and a noisy image xt, predict the noise (Œ≠) added to the image at step t-1.

xt ‚Üí Model ‚Üí Œ≠ (predicted noise). The model predicts (approximates) the noise added to x t-1 in the forward pass.

  


<https://github.com/Stability-AI/stablediffusion>

  


  


Stable Diffusion  
Created by StabilityAI. Stable Diffusion builds on high resolution image synthesis work using latent diffusion models by Rombach et al. It is the only diffusion based imaging model on this list that is completely open source.

As of this writing, Stable Diffusion v2.1 is available in the official StabilityAI repository.

Not only that, the open source developer community has been very active since its release. In a short period of time, the community has released several stable open source diffusion models, fine-tuned for various artistic styled datasets. You can freely use these models and create new images using these styles.

They can range from Japanese anime and futuristic robots to cyberpunk worlds. Just to intrigue your imagination, here are some examples of stable diffusion models.

The complete architecture of stable diffusion consists of three models:

A text encoder that accepts a text prompt.

Convert text hints to machine-readable vectors.

U-net

This is a diffusion model responsible for generating images.

A variational autoencoder consisting of an encoder and decoder model.

An encoder is used to reduce the size of an image. The UNet diffusion model works in this smaller dimension.

The decoder is then responsible for enhancing/restoring the image generated by the diffusion model to its original size.

One can easily access stable diffusion models using their DreamStudio platform. Creating an account will initially grant you 200 credits which you can use to play with the hints and create images of your choice.

Also, if you have the computing resources, you can also set up stable diffusion to run on your system by following the documentation in their repository.

<https://huggingface.co/spaces/sd-concepts-library/stable-diffusion-conceptualizer>

[**Lil‚ÄôLog**](https://lilianweng.github.io/ "Lil'Log (Alt + H)")

* [Posts](https://lilianweng.github.io/ "Posts")
* [Archive](https://lilianweng.github.io/archives "Archive")
* [Search](https://lilianweng.github.io/search/ "Search (Alt + /)")
* [Tags](https://lilianweng.github.io/tags/ "Tags")
* [FAQ](https://lilianweng.github.io/faq "FAQ")
* [emojisearch.app](https://www.emojisearch.app/ "emojisearch.app")

### What are Diffusion Models?

July 11, 2021 ¬∑ 26 min ¬∑ Lilian Weng

Table of Contents

[Updated on 2021‚Äì09‚Äì19: Highly recommend this blog post on [score-based generative modeling](https://yang-song.github.io/blog/2021/score/) by Yang Song (author of several key papers in the references)].  
[Updated on 2022‚Äì08‚Äì27: Added [classifier-free guidance](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#classifier-free-guidance), [GLIDE](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#glide), [unCLIP](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#unclip) and [Imagen](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#imagen).  
[Updated on 2022‚Äì08‚Äì31: Added [latent diffusion model](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#ldm).

So far, I‚Äôve written about three types of generative models, [GAN](https://lilianweng.github.io/posts/2017-08-20-gan/), [VAE](https://lilianweng.github.io/posts/2018-08-12-vae/), and [Flow-based](https://lilianweng.github.io/posts/2018-10-13-flow-models/) models. They have shown great success in generating high-quality samples, but each has some limitations of its own. GAN models are known for potentially unstable training and less diversity in generation due to their adversarial training nature. VAE relies on a surrogate loss. Flow models have to use specialized architectures to construct reversible transform.

Diffusion models are inspired by non-equilibrium thermodynamics. They define a Markov chain of diffusion steps to slowly add random noise to data and then learn to reverse the diffusion process to construct desired data samples from the noise. Unlike VAE or flow models, diffusion models are learned with a fixed procedure and the latent variable has high dimensionality (same as the original data).

![](https://cdn-images-1.medium.com/max/800/0*Ww8kTiIas-65dsOc.png)### What are Diffusion Models?

### 

Several diffusion-based generative models have been proposed with similar ideas underneath, including *diffusion probabilistic models* ([Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)), *noise-conditioned score network* (**NCSN**; [Yang & Ermon, 2019](https://arxiv.org/abs/1907.05600)), and *denoising diffusion probabilistic models* (**DDPM**; [Ho et al. 2020](https://arxiv.org/abs/2006.11239)).

### Forward diffusion process

### 

Given a data point sampled from a real data distribution ÔøΩ0‚àºÔøΩ(ÔøΩ), let us define a *forward diffusion process* in which we add small amount of Gaussian noise to the sample in ÔøΩ steps, producing a sequence of noisy samples ÔøΩ1,‚Ä¶,ÔøΩÔøΩ. The step sizes are controlled by a variance schedule {ÔøΩÔøΩ‚àà(0,1)}ÔøΩ=1ÔøΩ.

ÔøΩ(ÔøΩÔøΩ|ÔøΩÔøΩ‚àí1)=ÔøΩ(ÔøΩÔøΩ;1‚àíÔøΩÔøΩÔøΩÔøΩ‚àí1,ÔøΩÔøΩÔøΩ)ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)=‚àèÔøΩ=1ÔøΩÔøΩ(ÔøΩÔøΩ|ÔøΩÔøΩ‚àí1)

The data sample ÔøΩ0 gradually loses its distinguishable features as the step ÔøΩ becomes larger. Eventually when ÔøΩ‚Üí‚àû, ÔøΩÔøΩ is equivalent to an isotropic Gaussian distribution.

![](https://cdn-images-1.medium.com/max/800/0*WMsduHnv9BagImv3.png)A nice property of the above process is that we can sample ÔøΩÔøΩ at any arbitrary time step ÔøΩ in a closed form using [reparameterization trick](https://lilianweng.github.io/posts/2018-08-12-vae/#reparameterization-trick). Let ÔøΩÔøΩ=1‚àíÔøΩÔøΩ and ÔøΩ¬ØÔøΩ=‚àèÔøΩ=1ÔøΩÔøΩÔøΩ:

ÔøΩÔøΩ=ÔøΩÔøΩÔøΩÔøΩ‚àí1+1‚àíÔøΩÔøΩÔøΩÔøΩ‚àí1¬†;where ÔøΩÔøΩ‚àí1,ÔøΩÔøΩ‚àí2,‚ãØ‚àºÔøΩ(0,ÔøΩ)=ÔøΩÔøΩÔøΩÔøΩ‚àí1ÔøΩÔøΩ‚àí2+1‚àíÔøΩÔøΩÔøΩÔøΩ‚àí1ÔøΩ¬ØÔøΩ‚àí2¬†;where ÔøΩ¬ØÔøΩ‚àí2 merges two Gaussians (\*).=‚Ä¶=ÔøΩ¬ØÔøΩÔøΩ0+1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ(ÔøΩÔøΩ|ÔøΩ0)=ÔøΩ(ÔøΩÔøΩ;ÔøΩ¬ØÔøΩÔøΩ0,(1‚àíÔøΩ¬ØÔøΩ)ÔøΩ)

(\*) Recall that when we merge two Gaussians with different variance, ÔøΩ(0,ÔøΩ12ÔøΩ) and ÔøΩ(0,ÔøΩ22ÔøΩ), the new distribution is ÔøΩ(0,(ÔøΩ12+ÔøΩ22)ÔøΩ). Here the merged standard deviation is (1‚àíÔøΩÔøΩ)+ÔøΩÔøΩ(1‚àíÔøΩÔøΩ‚àí1)=1‚àíÔøΩÔøΩÔøΩÔøΩ‚àí1.

Usually, we can afford a larger update step when the sample gets noisier, so ÔøΩ1<ÔøΩ2<‚ãØ<ÔøΩÔøΩ and therefore ÔøΩ¬Ø1>‚ãØ>ÔøΩ¬ØÔøΩ.

### Connection with stochastic gradient Langevin¬†dynamics

### 

Langevin dynamics is a concept from physics, developed for statistically modeling molecular systems. Combined with stochastic gradient descent, *stochastic gradient Langevin dynamics* ([Welling & Teh 2011](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf)) can produce samples from a probability density ÔøΩ(ÔøΩ) using only the gradients ‚àáÔøΩlog‚Å°ÔøΩ(ÔøΩ) in a Markov chain of updates:

ÔøΩÔøΩ=ÔøΩÔøΩ‚àí1+ÔøΩ2‚àáÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ‚àí1)+ÔøΩÔøΩÔøΩ,where ÔøΩÔøΩ‚àºÔøΩ(0,ÔøΩ)

where ÔøΩ is the step size. When ÔøΩ‚Üí‚àû,ÔøΩ‚Üí0, ÔøΩÔøΩ equals to the true probability density ÔøΩ(ÔøΩ).

Compared to standard SGD, stochastic gradient Langevin dynamics injects Gaussian noise into the parameter updates to avoid collapses into local minima.

### Reverse diffusion process

### 

If we can reverse the above process and sample from ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ), we will be able to recreate the true sample from a Gaussian noise input, ÔøΩÔøΩ‚àºÔøΩ(0,ÔøΩ). Note that if ÔøΩÔøΩ is small enough, ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ) will also be Gaussian. Unfortunately, we cannot easily estimate ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ) because it needs to use the entire dataset and therefore we need to learn a model ÔøΩÔøΩ to approximate these conditional probabilities in order to run the *reverse diffusion process*.

ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)=ÔøΩ(ÔøΩÔøΩ)‚àèÔøΩ=1ÔøΩÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)=ÔøΩ(ÔøΩÔøΩ‚àí1;ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ),ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ))

![](https://cdn-images-1.medium.com/max/800/0*61YZeGDgwml3U1nR.png)It is noteworthy that the reverse conditional probability is tractable when conditioned on ÔøΩ0:

ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)=ÔøΩ(ÔøΩÔøΩ‚àí1;ÔøΩ~(ÔøΩÔøΩ,ÔøΩ0),ÔøΩ~ÔøΩÔøΩ)

Using Bayes‚Äô rule, we have:

ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)=ÔøΩ(ÔøΩÔøΩ|ÔøΩÔøΩ‚àí1,ÔøΩ0)ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩ0)ÔøΩ(ÔøΩÔøΩ|ÔøΩ0)‚àùexp‚Å°(‚àí12((ÔøΩÔøΩ‚àíÔøΩÔøΩÔøΩÔøΩ‚àí1)2ÔøΩÔøΩ+(ÔøΩÔøΩ‚àí1‚àíÔøΩ¬ØÔøΩ‚àí1ÔøΩ0)21‚àíÔøΩ¬ØÔøΩ‚àí1‚àí(ÔøΩÔøΩ‚àíÔøΩ¬ØÔøΩÔøΩ0)21‚àíÔøΩ¬ØÔøΩ))=exp‚Å°(‚àí12(ÔøΩÔøΩ2‚àí2ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ‚àí1+ÔøΩÔøΩÔøΩÔøΩ‚àí12ÔøΩÔøΩ+ÔøΩÔøΩ‚àí12‚àí2ÔøΩ¬ØÔøΩ‚àí1ÔøΩ0ÔøΩÔøΩ‚àí1+ÔøΩ¬ØÔøΩ‚àí1ÔøΩ021‚àíÔøΩ¬ØÔøΩ‚àí1‚àí(ÔøΩÔøΩ‚àíÔøΩ¬ØÔøΩÔøΩ0)21‚àíÔøΩ¬ØÔøΩ))=exp‚Å°(‚àí12((ÔøΩÔøΩÔøΩÔøΩ+11‚àíÔøΩ¬ØÔøΩ‚àí1)ÔøΩÔøΩ‚àí12‚àí(2ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ+2ÔøΩ¬ØÔøΩ‚àí11‚àíÔøΩ¬ØÔøΩ‚àí1ÔøΩ0)ÔøΩÔøΩ‚àí1+ÔøΩ(ÔøΩÔøΩ,ÔøΩ0)))

where ÔøΩ(ÔøΩÔøΩ,ÔøΩ0) is some function not involving ÔøΩÔøΩ‚àí1 and details are omitted. Following the standard Gaussian density function, the mean and variance can be parameterized as follows (recall that ÔøΩÔøΩ=1‚àíÔøΩÔøΩ and ÔøΩ¬ØÔøΩ=‚àèÔøΩ=1ÔøΩÔøΩÔøΩ):

ÔøΩ~ÔøΩ=1/(ÔøΩÔøΩÔøΩÔøΩ+11‚àíÔøΩ¬ØÔøΩ‚àí1)=1/(ÔøΩÔøΩ‚àíÔøΩ¬ØÔøΩ+ÔøΩÔøΩÔøΩÔøΩ(1‚àíÔøΩ¬ØÔøΩ‚àí1))=1‚àíÔøΩ¬ØÔøΩ‚àí11‚àíÔøΩ¬ØÔøΩ‚ãÖÔøΩÔøΩÔøΩ~ÔøΩ(ÔøΩÔøΩ,ÔøΩ0)=(ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ+ÔøΩ¬ØÔøΩ‚àí11‚àíÔøΩ¬ØÔøΩ‚àí1ÔøΩ0)/(ÔøΩÔøΩÔøΩÔøΩ+11‚àíÔøΩ¬ØÔøΩ‚àí1)=(ÔøΩÔøΩÔøΩÔøΩÔøΩÔøΩ+ÔøΩ¬ØÔøΩ‚àí11‚àíÔøΩ¬ØÔøΩ‚àí1ÔøΩ0)1‚àíÔøΩ¬ØÔøΩ‚àí11‚àíÔøΩ¬ØÔøΩ‚ãÖÔøΩÔøΩ=ÔøΩÔøΩ(1‚àíÔøΩ¬ØÔøΩ‚àí1)1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ+ÔøΩ¬ØÔøΩ‚àí1ÔøΩÔøΩ1‚àíÔøΩ¬ØÔøΩÔøΩ0

Thanks to the [nice property](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice), we can represent ÔøΩ0=1ÔøΩ¬ØÔøΩ(ÔøΩÔøΩ‚àí1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ) and plug it into the above equation and obtain:

ÔøΩ~ÔøΩ=ÔøΩÔøΩ(1‚àíÔøΩ¬ØÔøΩ‚àí1)1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ+ÔøΩ¬ØÔøΩ‚àí1ÔøΩÔøΩ1‚àíÔøΩ¬ØÔøΩ1ÔøΩ¬ØÔøΩ(ÔøΩÔøΩ‚àí1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ)=1ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1‚àíÔøΩÔøΩ1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ)

As demonstrated in Fig. 2., such a setup is very similar to [VAE](https://lilianweng.github.io/posts/2018-08-12-vae/) and thus we can use the variational lower bound to optimize the negative log-likelihood.

‚àílog‚Å°ÔøΩÔøΩ(ÔøΩ0)‚â§‚àílog‚Å°ÔøΩÔøΩ(ÔøΩ0)+ÔøΩKL(ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)‚ÄñÔøΩÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0))=‚àílog‚Å°ÔøΩÔøΩ(ÔøΩ0)+ÔøΩÔøΩ1:ÔøΩ‚àºÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)[log‚Å°ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)/ÔøΩÔøΩ(ÔøΩ0)]=‚àílog‚Å°ÔøΩÔøΩ(ÔøΩ0)+ÔøΩÔøΩ[log‚Å°ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)+log‚Å°ÔøΩÔøΩ(ÔøΩ0)]=ÔøΩÔøΩ[log‚Å°ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)]Let ÔøΩVLB=ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)[log‚Å°ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)]‚â•‚àíÔøΩÔøΩ(ÔøΩ0)log‚Å°ÔøΩÔøΩ(ÔøΩ0)

It is also straightforward to get the same result using Jensen‚Äôs inequality. Say we want to minimize the cross entropy as the learning objective,

ÔøΩCE=‚àíÔøΩÔøΩ(ÔøΩ0)log‚Å°ÔøΩÔøΩ(ÔøΩ0)=‚àíÔøΩÔøΩ(ÔøΩ0)log‚Å°(‚à´ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)ÔøΩÔøΩ1:ÔøΩ)=‚àíÔøΩÔøΩ(ÔøΩ0)log‚Å°(‚à´ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ1:ÔøΩ)=‚àíÔøΩÔøΩ(ÔøΩ0)log‚Å°(ÔøΩÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0))‚â§‚àíÔøΩÔøΩ(ÔøΩ0:ÔøΩ)log‚Å°ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)=ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)[log‚Å°ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)]=ÔøΩVLB

To convert each term in the equation to be analytically computable, the objective can be further rewritten to be a combination of several KL-divergence and entropy terms (See the detailed step-by-step process in Appendix B in [Sohl-Dickstein et al., 2015](https://arxiv.org/abs/1503.03585)):

ÔøΩVLB=ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)[log‚Å°ÔøΩ(ÔøΩ1:ÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0:ÔøΩ)]=ÔøΩÔøΩ[log‚Å°‚àèÔøΩ=1ÔøΩÔøΩ(ÔøΩÔøΩ|ÔøΩÔøΩ‚àí1)ÔøΩÔøΩ(ÔøΩÔøΩ)‚àèÔøΩ=1ÔøΩÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)]=ÔøΩÔøΩ[‚àílog‚Å°ÔøΩÔøΩ(ÔøΩÔøΩ)+‚àëÔøΩ=1ÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ|ÔøΩÔøΩ‚àí1)ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)]=ÔøΩÔøΩ[‚àílog‚Å°ÔøΩÔøΩ(ÔøΩÔøΩ)+‚àëÔøΩ=2ÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ|ÔøΩÔøΩ‚àí1)ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)+log‚Å°ÔøΩ(ÔøΩ1|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0|ÔøΩ1)]=ÔøΩÔøΩ[‚àílog‚Å°ÔøΩÔøΩ(ÔøΩÔøΩ)+‚àëÔøΩ=2ÔøΩlog‚Å°(ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)‚ãÖÔøΩ(ÔøΩÔøΩ|ÔøΩ0)ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩ0))+log‚Å°ÔøΩ(ÔøΩ1|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0|ÔøΩ1)]=ÔøΩÔøΩ[‚àílog‚Å°ÔøΩÔøΩ(ÔøΩÔøΩ)+‚àëÔøΩ=2ÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)+‚àëÔøΩ=2ÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ|ÔøΩ0)ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩ0)+log‚Å°ÔøΩ(ÔøΩ1|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0|ÔøΩ1)]=ÔøΩÔøΩ[‚àílog‚Å°ÔøΩÔøΩ(ÔøΩÔøΩ)+‚àëÔøΩ=2ÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)+log‚Å°ÔøΩ(ÔøΩÔøΩ|ÔøΩ0)ÔøΩ(ÔøΩ1|ÔøΩ0)+log‚Å°ÔøΩ(ÔøΩ1|ÔøΩ0)ÔøΩÔøΩ(ÔøΩ0|ÔøΩ1)]=ÔøΩÔøΩ[log‚Å°ÔøΩ(ÔøΩÔøΩ|ÔøΩ0)ÔøΩÔøΩ(ÔøΩÔøΩ)+‚àëÔøΩ=2ÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)‚àílog‚Å°ÔøΩÔøΩ(ÔøΩ0|ÔøΩ1)]=ÔøΩÔøΩ[ÔøΩKL(ÔøΩ(ÔøΩÔøΩ|ÔøΩ0)‚à•ÔøΩÔøΩ(ÔøΩÔøΩ))‚èüÔøΩÔøΩ+‚àëÔøΩ=2ÔøΩÔøΩKL(ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)‚à•ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ))‚èüÔøΩÔøΩ‚àí1‚àílog‚Å°ÔøΩÔøΩ(ÔøΩ0|ÔøΩ1)‚èüÔøΩ0]

Let‚Äôs label each component in the variational lower bound loss separately:

ÔøΩVLB=ÔøΩÔøΩ+ÔøΩÔøΩ‚àí1+‚ãØ+ÔøΩ0where ÔøΩÔøΩ=ÔøΩKL(ÔøΩ(ÔøΩÔøΩ|ÔøΩ0)‚à•ÔøΩÔøΩ(ÔøΩÔøΩ))ÔøΩÔøΩ=ÔøΩKL(ÔøΩ(ÔøΩÔøΩ|ÔøΩÔøΩ+1,ÔøΩ0)‚à•ÔøΩÔøΩ(ÔøΩÔøΩ|ÔøΩÔøΩ+1)) for 1‚â§ÔøΩ‚â§ÔøΩ‚àí1ÔøΩ0=‚àílog‚Å°ÔøΩÔøΩ(ÔøΩ0|ÔøΩ1)

Every KL term in ÔøΩVLB (except for ÔøΩ0) compares two Gaussian distributions and therefore they can be computed in [closed form](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Multivariate_normal_distributions). ÔøΩÔøΩ is constant and can be ignored during training because ÔøΩ has no learnable parameters and ÔøΩÔøΩ is a Gaussian noise. [Ho et al. 2020](https://arxiv.org/abs/2006.11239) models ÔøΩ0 using a separate discrete decoder derived from ÔøΩ(ÔøΩ0;ÔøΩÔøΩ(ÔøΩ1,1),ÔøΩÔøΩ(ÔøΩ1,1)).

### Parameterization of ÔøΩÔøΩ for Training¬†Loss

### 

Recall that we need to learn a neural network to approximate the conditioned probability distributions in the reverse diffusion process, ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ)=ÔøΩ(ÔøΩÔøΩ‚àí1;ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ),ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)). We would like to train ÔøΩÔøΩ to predict ÔøΩ~ÔøΩ=1ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1‚àíÔøΩÔøΩ1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ). Because ÔøΩÔøΩ is available as input at training time, we can reparameterize the Gaussian noise term instead to make it predict ÔøΩÔøΩ from the input ÔøΩÔøΩ at time step ÔøΩ:

ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)=1ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1‚àíÔøΩÔøΩ1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ))Thus ÔøΩÔøΩ‚àí1=ÔøΩ(ÔøΩÔøΩ‚àí1;1ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1‚àíÔøΩÔøΩ1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)),ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ))

The loss term ÔøΩÔøΩ is parameterized to minimize the difference from ÔøΩ~¬†:

ÔøΩÔøΩ=ÔøΩÔøΩ0,ÔøΩ[12‚ÄñÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚Äñ22‚ÄñÔøΩ~ÔøΩ(ÔøΩÔøΩ,ÔøΩ0)‚àíÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚Äñ2]=ÔøΩÔøΩ0,ÔøΩ[12‚ÄñÔøΩÔøΩ‚Äñ22‚Äñ1ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1‚àíÔøΩÔøΩ1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ)‚àí1ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1‚àíÔøΩÔøΩ1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ))‚Äñ2]=ÔøΩÔøΩ0,ÔøΩ[(1‚àíÔøΩÔøΩ)22ÔøΩÔøΩ(1‚àíÔøΩ¬ØÔøΩ)‚ÄñÔøΩÔøΩ‚Äñ22‚ÄñÔøΩÔøΩ‚àíÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚Äñ2]=ÔøΩÔøΩ0,ÔøΩ[(1‚àíÔøΩÔøΩ)22ÔøΩÔøΩ(1‚àíÔøΩ¬ØÔøΩ)‚ÄñÔøΩÔøΩ‚Äñ22‚ÄñÔøΩÔøΩ‚àíÔøΩÔøΩ(ÔøΩ¬ØÔøΩÔøΩ0+1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ,ÔøΩ)‚Äñ2]

### Simplification

### 

Empirically, [Ho et al. (2020)](https://arxiv.org/abs/2006.11239) found that training the diffusion model works better with a simplified objective that ignores the weighting term:

ÔøΩÔøΩsimple=ÔøΩÔøΩ‚àº[1,ÔøΩ],ÔøΩ0,ÔøΩÔøΩ[‚ÄñÔøΩÔøΩ‚àíÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚Äñ2]=ÔøΩÔøΩ‚àº[1,ÔøΩ],ÔøΩ0,ÔøΩÔøΩ[‚ÄñÔøΩÔøΩ‚àíÔøΩÔøΩ(ÔøΩ¬ØÔøΩÔøΩ0+1‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ,ÔøΩ)‚Äñ2]

The final simple objective is:

ÔøΩsimple=ÔøΩÔøΩsimple+ÔøΩ

where ÔøΩ is a constant not depending on ÔøΩ.

![](https://cdn-images-1.medium.com/max/800/0*aHBlPIYD2OJPFL41.png)### Connection with noise-conditioned score networks¬†(NCSN)

### 

[Song & Ermon (2019)](https://arxiv.org/abs/1907.05600) proposed a score-based generative modeling method where samples are produced via [Langevin dynamics](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#connection-with-stochastic-gradient-langevin-dynamics) using gradients of the data distribution estimated with score matching. The score of each sample ÔøΩ‚Äôs density probability is defined as its gradient ‚àáÔøΩlog‚Å°ÔøΩ(ÔøΩ). A score network ÔøΩÔøΩ:ÔøΩÔøΩ‚ÜíÔøΩÔøΩ is trained to estimate it, ÔøΩÔøΩ(ÔøΩ)‚âà‚àáÔøΩlog‚Å°ÔøΩ(ÔøΩ).

To make it scalable with high-dimensional data in the deep learning setting, they proposed to use either *denoising score matching* ([Vincent, 2011](http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf)) or *sliced score matching* (use random projections; [Song et al., 2019](https://arxiv.org/abs/1905.07088)). Denosing score matching adds a pre-specified small noise to the data ÔøΩ(ÔøΩ~|ÔøΩ) and estimates ÔøΩ(ÔøΩ~) with score matching.

Recall that Langevin dynamics can sample data points from a probability density distribution using only the score ‚àáÔøΩlog‚Å°ÔøΩ(ÔøΩ) in an iterative process.

However, according to the manifold hypothesis, most of the data is expected to concentrate in a low dimensional manifold, even though the observed data might look only arbitrarily high-dimensional. It brings a negative effect on score estimation since the data points cannot cover the whole space. In regions where data density is low, the score estimation is less reliable. After adding a small Gaussian noise to make the perturbed data distribution cover the full space ÔøΩÔøΩ, the training of the score estimator network becomes more stable. [Song & Ermon (2019)](https://arxiv.org/abs/1907.05600) improved it by perturbing the data with the noise of *different levels* and train a noise-conditioned score network to *jointly* estimate the scores of all the perturbed data at different noise levels.

The schedule of increasing noise levels resembles the forward diffusion process. If we use the diffusion process annotation, the score approximates ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚âà‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ). Given a Gaussian distribution ÔøΩ‚àºÔøΩ(ÔøΩ,ÔøΩ2ÔøΩ), we can write the derivative of the logarithm of its density function as ‚àáÔøΩlog‚Å°ÔøΩ(ÔøΩ)=‚àáÔøΩ(‚àí12ÔøΩ2(ÔøΩ‚àíÔøΩ)2)=‚àíÔøΩ‚àíÔøΩÔøΩ2=‚àíÔøΩÔøΩ where ÔøΩ‚àºÔøΩ(0,ÔøΩ). [Recall](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice) that ÔøΩ(ÔøΩÔøΩ|ÔøΩ0)‚àºÔøΩ(ÔøΩ¬ØÔøΩÔøΩ0,(1‚àíÔøΩ¬ØÔøΩ)ÔøΩ) and therefore,

ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚âà‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ)=ÔøΩÔøΩ(ÔøΩ0)[‚àáÔøΩÔøΩÔøΩ(ÔøΩÔøΩ|ÔøΩ0)]=ÔøΩÔøΩ(ÔøΩ0)[‚àíÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)1‚àíÔøΩ¬ØÔøΩ]=‚àíÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)1‚àíÔøΩ¬ØÔøΩ

### Parameterization of¬†ÔøΩÔøΩ

### 

The forward variances are set to be a sequence of linearly increasing constants in [Ho et al. (2020)](https://arxiv.org/abs/2006.11239), from ÔøΩ1=10‚àí4 to ÔøΩÔøΩ=0.02. They are relatively small compared to the normalized image pixel values between [‚àí1,1]. Diffusion models in their experiments showed high-quality samples but still could not achieve competitive model log-likelihood as other generative models.

[Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) proposed several improvement techniques to help diffusion models to obtain lower NLL. One of the improvements is to use a cosine-based variance schedule. The choice of the scheduling function can be arbitrary, as long as it provides a near-linear drop in the middle of the training process and subtle changes around ÔøΩ=0 and ÔøΩ=ÔøΩ.

ÔøΩÔøΩ=clip(1‚àíÔøΩ¬ØÔøΩÔøΩ¬ØÔøΩ‚àí1,0.999)ÔøΩ¬ØÔøΩ=ÔøΩ(ÔøΩ)ÔøΩ(0)where ÔøΩ(ÔøΩ)=cos‚Å°(ÔøΩ/ÔøΩ+ÔøΩ1+ÔøΩ‚ãÖÔøΩ2)2

where the small offset ÔøΩ is to prevent ÔøΩÔøΩ from being too small when close to ÔøΩ=0.

![](https://cdn-images-1.medium.com/max/800/0*MavQcFyBGx2J2v40.png)### Parameterization of reverse process variance¬†ÔøΩÔøΩ

### 

[Ho et al. (2020)](https://arxiv.org/abs/2006.11239) chose to fix ÔøΩÔøΩ as constants instead of making them learnable and set ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)=ÔøΩÔøΩ2ÔøΩ¬†, where ÔøΩÔøΩ is not learned but set to ÔøΩÔøΩ or ÔøΩ~ÔøΩ=1‚àíÔøΩ¬ØÔøΩ‚àí11‚àíÔøΩ¬ØÔøΩ‚ãÖÔøΩÔøΩ. Because they found that learning a diagonal variance ÔøΩÔøΩ leads to unstable training and poorer sample quality.

[Nichol & Dhariwal (2021)](https://arxiv.org/abs/2102.09672) proposed to learn ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ) as an interpolation between ÔøΩÔøΩ and ÔøΩ~ÔøΩ by model predicting a mixing vector ÔøΩ¬†:

ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)=exp‚Å°(ÔøΩlog‚Å°ÔøΩÔøΩ+(1‚àíÔøΩ)log‚Å°ÔøΩ~ÔøΩ)

However, the simple objective ÔøΩsimple does not depend on ÔøΩÔøΩ¬†. To add the dependency, they constructed a hybrid objective ÔøΩhybrid=ÔøΩsimple+ÔøΩÔøΩVLB where ÔøΩ=0.001 is small and stop gradient on ÔøΩÔøΩ in the ÔøΩVLB term such that ÔøΩVLB only guides the learning of ÔøΩÔøΩ. Empirically they observed that ÔøΩVLB is pretty challenging to optimize likely due to noisy gradients, so they proposed to use a time-averaging smoothed version of ÔøΩVLB with importance sampling.

![](https://cdn-images-1.medium.com/max/800/0*s3KcCXGNC5qPfBZk.png)### Speed up Diffusion Model¬†Sampling

### 

It is very slow to generate a sample from DDPM by following the Markov chain of the reverse diffusion process, as ÔøΩ can be up to one or a few thousand steps. One data point from [Song et al. 2020](https://arxiv.org/abs/2010.02502): ‚ÄúFor example, it takes around 20 hours to sample 50k images of size 32 √ó 32 from a DDPM, but less than a minute to do so from a GAN on an Nvidia 2080 Ti GPU.‚Äù

One simple way is to run a strided sampling schedule ([Nichol & Dhariwal, 2021](https://arxiv.org/abs/2102.09672)) by taking the sampling update every ‚åàÔøΩ/ÔøΩ‚åâ steps to reduce the process from ÔøΩ to ÔøΩ steps. The new sampling schedule for generation is {ÔøΩ1,‚Ä¶,ÔøΩÔøΩ} where ÔøΩ1<ÔøΩ2<‚ãØ<ÔøΩÔøΩ‚àà[1,ÔøΩ] and ÔøΩ<ÔøΩ.

For another approach, let‚Äôs rewrite ÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0) to be parameterized by a desired standard deviation ÔøΩÔøΩ according to the [nice property](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#nice):

ÔøΩÔøΩ‚àí1=ÔøΩ¬ØÔøΩ‚àí1ÔøΩ0+1‚àíÔøΩ¬ØÔøΩ‚àí1ÔøΩÔøΩ‚àí1=ÔøΩ¬ØÔøΩ‚àí1ÔøΩ0+1‚àíÔøΩ¬ØÔøΩ‚àí1‚àíÔøΩÔøΩ2ÔøΩÔøΩ+ÔøΩÔøΩÔøΩ=ÔøΩ¬ØÔøΩ‚àí1ÔøΩ0+1‚àíÔøΩ¬ØÔøΩ‚àí1‚àíÔøΩÔøΩ2ÔøΩÔøΩ‚àíÔøΩ¬ØÔøΩÔøΩ01‚àíÔøΩ¬ØÔøΩ+ÔøΩÔøΩÔøΩÔøΩÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)=ÔøΩ(ÔøΩÔøΩ‚àí1;ÔøΩ¬ØÔøΩ‚àí1ÔøΩ0+1‚àíÔøΩ¬ØÔøΩ‚àí1‚àíÔøΩÔøΩ2ÔøΩÔøΩ‚àíÔøΩ¬ØÔøΩÔøΩ01‚àíÔøΩ¬ØÔøΩ,ÔøΩÔøΩ2ÔøΩ)

Recall that in ÔøΩ(ÔøΩÔøΩ‚àí1|ÔøΩÔøΩ,ÔøΩ0)=ÔøΩ(ÔøΩÔøΩ‚àí1;ÔøΩ~(ÔøΩÔøΩ,ÔøΩ0),ÔøΩ~ÔøΩÔøΩ), therefore we have:

ÔøΩ~ÔøΩ=ÔøΩÔøΩ2=1‚àíÔøΩ¬ØÔøΩ‚àí11‚àíÔøΩ¬ØÔøΩ‚ãÖÔøΩÔøΩ

Let ÔøΩÔøΩ2=ÔøΩ‚ãÖÔøΩ~ÔøΩ such that we can adjust ÔøΩ‚ààÔøΩ+ as a hyperparameter to control the sampling stochasticity. The special case of ÔøΩ=0 makes the sampling process *deterministic*. Such a model is named the *denoising diffusion implicit model* (**DDIM**; [Song et al., 2020](https://arxiv.org/abs/2010.02502)). DDIM has the same marginal noise distribution but deterministically maps noise back to the original data samples.

During generation, we only sample a subset of ÔøΩ diffusion steps {ÔøΩ1,‚Ä¶,ÔøΩÔøΩ} and the inference process becomes:

ÔøΩÔøΩ,ÔøΩ(ÔøΩÔøΩÔøΩ‚àí1|ÔøΩÔøΩÔøΩ,ÔøΩ0)=ÔøΩ(ÔøΩÔøΩÔøΩ‚àí1;ÔøΩ¬ØÔøΩ‚àí1ÔøΩ0+1‚àíÔøΩ¬ØÔøΩ‚àí1‚àíÔøΩÔøΩ2ÔøΩÔøΩÔøΩ‚àíÔøΩ¬ØÔøΩÔøΩ01‚àíÔøΩ¬ØÔøΩ,ÔøΩÔøΩ2ÔøΩ)

While all the models are trained with ÔøΩ=1000 diffusion steps in the experiments, they observed that DDIM (ÔøΩ=0) can produce the best quality samples when ÔøΩ is small, while DDPM (ÔøΩ=1) performs much worse on small ÔøΩ. DDPM does perform better when we can afford to run the full reverse Markov diffusion steps (ÔøΩ=ÔøΩ=1000). With DDIM, it is possible to train the diffusion model up to any arbitrary number of forward steps but only sample from a subset of steps in the generative process.

![](https://cdn-images-1.medium.com/max/800/0*riGclOZ7j9SstxXb.png)Compared to DDPM, DDIM is able to:

1. Generate higher-quality samples using a much fewer number of steps.
2. Have ‚Äúconsistency‚Äù property since the generative process is deterministic, meaning that multiple samples conditioned on the same latent variable should have similar high-level features.
3. Because of the consistency, DDIM can do semantically meaningful interpolation in the latent variable.

*Latent diffusion model* (**LDM**; [Rombach & Blattmann, et al. 2022](https://arxiv.org/abs/2112.10752)) runs the diffusion process in the latent space instead of pixel space, making training cost lower and inference speed faster. It is motivated by the observation that most bits of an image contribute to perceptual details and the semantic and conceptual composition still remains after aggressive compression. LDM loosely decomposes the perceptual compression and semantic compression with generative modeling learning by first trimming off pixel-level redundancy with autoencoder and then manipulate/generate semantic concepts with diffusion process on learned latent.

![](https://cdn-images-1.medium.com/max/800/0*2cJD5Za96QdQJNQJ.png)The perceptual compression process relies on an autoencoder model. An encoder ÔøΩ is used to compress the input image ÔøΩ‚ààÔøΩÔøΩ√óÔøΩ√ó3 to a smaller 2D latent vector ÔøΩ=ÔøΩ(ÔøΩ)‚ààÔøΩ‚Ñé√óÔøΩ√óÔøΩ¬†, where the downsampling rate ÔøΩ=ÔøΩ/‚Ñé=ÔøΩ/ÔøΩ=2ÔøΩ,ÔøΩ‚ààÔøΩ. Then an decoder ÔøΩ reconstructs the images from the latent vector, ÔøΩ~=ÔøΩ(ÔøΩ). The paper explored two types of regularization in autoencoder training to avoid arbitrarily high-variance in the latent spaces.

* KL-reg: A small KL penalty towards a standard normal distribution over the learned latent, similar to [VAE](https://lilianweng.github.io/posts/2018-08-12-vae/).
* VQ-reg: Uses a vector quantization layer within the decoder, like [VQVAE](https://lilianweng.github.io/posts/2018-08-12-vae/#vq-vae-and-vq-vae-2) but the quantization layer is absorbed by the decoder.

The diffusion and denoising processes happen on the latent vector ÔøΩ. The denoising model is a time-conditioned U-Net, augmented with the cross-attention mechanism to handle flexible conditioning information for image generation (e.g. class labels, semantic maps, blurred variants of an image). The design is equivalent to fuse representation of different modality into the model with cross-attention mechanism. Each type of conditioning information is paired with a domain-specific encoder ÔøΩÔøΩ to project the conditioning input ÔøΩ to an intermediate representation that can be mapped into cross-attention component, ÔøΩÔøΩ(ÔøΩ)‚ààÔøΩÔøΩ√óÔøΩÔøΩ:

Attention(ÔøΩ,ÔøΩ,ÔøΩ)=softmax(ÔøΩÔøΩ‚ä§ÔøΩ)‚ãÖÔøΩwhere ÔøΩ=ÔøΩÔøΩ(ÔøΩ)‚ãÖÔøΩÔøΩ(ÔøΩÔøΩ),ÔøΩ=ÔøΩÔøΩ(ÔøΩ)‚ãÖÔøΩÔøΩ(ÔøΩ),ÔøΩ=ÔøΩÔøΩ(ÔøΩ)‚ãÖÔøΩÔøΩ(ÔøΩ)and ÔøΩÔøΩ(ÔøΩ)‚ààÔøΩÔøΩ√óÔøΩÔøΩÔøΩ,ÔøΩÔøΩ(ÔøΩ),ÔøΩÔøΩ(ÔøΩ)‚ààÔøΩÔøΩ√óÔøΩÔøΩ,ÔøΩÔøΩ(ÔøΩÔøΩ)‚ààÔøΩÔøΩ√óÔøΩÔøΩÔøΩ,ÔøΩÔøΩ(ÔøΩ)‚ààÔøΩÔøΩ√óÔøΩÔøΩ

![](https://cdn-images-1.medium.com/max/800/0*ia-MjXlmIZ6INZxS.png)### Conditioned Generation

### 

While training generative models on images with conditioning information such as ImageNet dataset, it is common to generate samples conditioned on class labels or a piece of descriptive text.

### Classifier Guided Diffusion

### 

To explicit incorporate class information into the diffusion process, [Dhariwal & Nichol (2021)](https://arxiv.org/abs/2105.05233) trained a classifier ÔøΩÔøΩ(ÔøΩ|ÔøΩÔøΩ,ÔøΩ) on noisy image ÔøΩÔøΩ and use gradients ‚àáÔøΩlog‚Å°ÔøΩÔøΩ(ÔøΩ|ÔøΩÔøΩ) to guide the diffusion sampling process toward the conditioning information ÔøΩ (e.g. a target class label) by altering the noise prediction. [Recall](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#score) that ‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ)=‚àí11‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ) and we can write the score function for the joint distribution ÔøΩ(ÔøΩÔøΩ,ÔøΩ) as following,

‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ,ÔøΩ)=‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ)+‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩ|ÔøΩÔøΩ)‚âà‚àí11‚àíÔøΩ¬ØÔøΩÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)+‚àáÔøΩÔøΩlog‚Å°ÔøΩÔøΩ(ÔøΩ|ÔøΩÔøΩ)=‚àí11‚àíÔøΩ¬ØÔøΩ(ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚àí1‚àíÔøΩ¬ØÔøΩ‚àáÔøΩÔøΩlog‚Å°ÔøΩÔøΩ(ÔøΩ|ÔøΩÔøΩ))

Thus, a new classifier-guided predictor ÔøΩ¬ØÔøΩ would take the form as following,

ÔøΩ¬ØÔøΩ(ÔøΩÔøΩ,ÔøΩ)=ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚àí1‚àíÔøΩ¬ØÔøΩ‚àáÔøΩÔøΩlog‚Å°ÔøΩÔøΩ(ÔøΩ|ÔøΩÔøΩ)

To control the strength of the classifier guidance, we can add a weight ÔøΩ to the delta part,

ÔøΩ¬ØÔøΩ(ÔøΩÔøΩ,ÔøΩ)=ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)‚àí1‚àíÔøΩ¬ØÔøΩÔøΩ‚àáÔøΩÔøΩlog‚Å°ÔøΩÔøΩ(ÔøΩ|ÔøΩÔøΩ)

The resulting *ablated diffusion model* (**ADM**) and the one with additional classifier guidance (**ADM-G**) are able to achieve better results than SOTA generative models (e.g. BigGAN).

![](https://cdn-images-1.medium.com/max/800/0*bxKRprZyS4mwV2Sa.png)Additionally with some modifications on the U-Net architecture, [Dhariwal & Nichol (2021)](https://arxiv.org/abs/2105.05233) showed performance better than GAN with diffusion models. The architecture modifications include larger model depth/width, more attention heads, multi-resolution attention, BigGAN residual blocks for up/downsampling, residual connection rescale by 1/2 and adaptive group normalization (AdaGN).

### Classifier-Free Guidance

### 

Without an independent classifier ÔøΩÔøΩ, it is still possible to run conditional diffusion steps by incorporating the scores from a conditional and an unconditional diffusion model ([Ho & Salimans, 2021](https://openreview.net/forum?id=qw8AKxfYbI)). Let unconditional denoising diffusion model ÔøΩÔøΩ(ÔøΩ) parameterized through a score estimator ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ) and the conditional model ÔøΩÔøΩ(ÔøΩ|ÔøΩ) parameterized through ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ,ÔøΩ). These two models can be learned via a single neural network. Precisely, a conditional diffusion model ÔøΩÔøΩ(ÔøΩ|ÔøΩ) is trained on paired data (ÔøΩ,ÔøΩ), where the conditioning information ÔøΩ gets discarded periodically at random such that the model knows how to generate images unconditionally as well, i.e. ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)=ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ,ÔøΩ=‚àÖ).

The gradient of an implicit classifier can be represented with conditional and unconditional score estimators. Once plugged into the classifier-guided modified score, the score contains no dependency on a separate classifier.

‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩ|ÔøΩÔøΩ)=‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ|ÔøΩ)‚àí‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩÔøΩ)=‚àí11‚àíÔøΩ¬ØÔøΩ(ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ,ÔøΩ)‚àíÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ))ÔøΩ¬ØÔøΩ(ÔøΩÔøΩ,ÔøΩ,ÔøΩ)=ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ,ÔøΩ)‚àí1‚àíÔøΩ¬ØÔøΩÔøΩ‚àáÔøΩÔøΩlog‚Å°ÔøΩ(ÔøΩ|ÔøΩÔøΩ)=ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ,ÔøΩ)+ÔøΩ(ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ,ÔøΩ)‚àíÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ))=(ÔøΩ+1)ÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ,ÔøΩ)‚àíÔøΩÔøΩÔøΩ(ÔøΩÔøΩ,ÔøΩ)

Their experiments showed that classifier-free guidance can achieve a good balance between FID (distinguish between synthetic and generated images) and IS (quality and diversity).

The guided diffusion model, GLIDE ([Nichol, Dhariwal & Ramesh, et al. 2022](https://arxiv.org/abs/2112.10741)), explored both guiding strategies, CLIP guidance and classifier-free guidance, and found that the latter is more preferred. They hypothesized that it is because CLIP guidance exploits the model with adversarial examples towards the CLIP model, rather than optimize the better matched images generation.

### Scale up Generation Resolution and¬†Quality

### 

To generate high-quality images at high resolution, [Ho et al. (2021)](https://arxiv.org/abs/2106.15282) proposed to use a pipeline of multiple diffusion models at increasing resolutions. *Noise conditioning augmentation* between pipeline models is crucial to the final image quality, which is to apply strong data augmentation to the conditioning input ÔøΩ of each super-resolution model ÔøΩÔøΩ(ÔøΩ|ÔøΩ). The conditioning noise helps reduce compounding error in the pipeline setup. *U-net* is a common choice of model architecture in diffusion modeling for high-resolution image generation.

![](https://cdn-images-1.medium.com/max/800/0*RKULSx8V9b5w9NOO.png)They found the most effective noise is to apply Gaussian noise at low resolution and Gaussian blur at high resolution. In addition, they also explored two forms of conditioning augmentation that require small modification to the training process. Note that conditioning noise is only applied to training but not at inference.

* Truncated conditioning augmentation stops the diffusion process early at step ÔøΩ>0 for low resolution.
* Non-truncated conditioning augmentation runs the full low resolution reverse process until step 0 but then corrupt it by ÔøΩÔøΩ‚àºÔøΩ(ÔøΩÔøΩ|ÔøΩ0) and then feeds the corrupted ÔøΩÔøΩ s into the super-resolution model.

The two-stage diffusion model **unCLIP** ([Ramesh et al. 2022](https://arxiv.org/abs/2204.06125)) heavily utilizes the CLIP text encoder to produce text-guided images at high quality. Given a pretrained CLIP model ÔøΩ and paired training data for the diffusion model, (ÔøΩ,ÔøΩ), where ÔøΩ is an image and ÔøΩ is the corresponding caption, we can compute the CLIP text and image embedding, ÔøΩÔøΩ(ÔøΩ) and ÔøΩÔøΩ(ÔøΩ), respectively. The unCLIP learns two models in parallel:

* A prior model ÔøΩ(ÔøΩÔøΩ|ÔøΩ): outputs CLIP image embedding ÔøΩÔøΩ given the text ÔøΩ.
* A decoder ÔøΩ(ÔøΩ|ÔøΩÔøΩ,[ÔøΩ]): generates the image ÔøΩ given CLIP image embedding ÔøΩÔøΩ and optionally the original text ÔøΩ.

These two models enable conditional generation, because

ÔøΩ(ÔøΩ|ÔøΩ)=ÔøΩ(ÔøΩ,ÔøΩÔøΩ|ÔøΩ)‚èüÔøΩÔøΩ is deterministic given ÔøΩ=ÔøΩ(ÔøΩ|ÔøΩÔøΩ,ÔøΩ)ÔøΩ(ÔøΩÔøΩ|ÔøΩ)

![](https://cdn-images-1.medium.com/max/800/0*i-dKwV3wi9mUbksw.png)unCLIP follows a two-stage image generation process:

1. Given a text ÔøΩ, a CLIP model is first used to generate a text embedding ÔøΩÔøΩ(ÔøΩ). Using CLIP latent space enables zero-shot image manipulation via text.
2. A diffusion or autoregressive prior ÔøΩ(ÔøΩÔøΩ|ÔøΩ) processes this CLIP text embedding to construct an image prior and then a diffusion decoder ÔøΩ(ÔøΩ|ÔøΩÔøΩ,[ÔøΩ]) generates an image, conditioned on the prior. This decoder can also generate image variations conditioned on an image input, preserving its style and semantics.

Instead of CLIP model, **Imagen** ([Saharia et al. 2022](https://arxiv.org/abs/2205.11487)) uses a pre-trained large LM (i.e. a frozen T5-XXL text encoder) to encode text for image generation. There is a general trend that larger model size can lead to better image quality and text-image alignment. They found that T5-XXL and CLIP text encoder achieve similar performance on MS-COCO, but human evaluation prefers T5-XXL on DrawBench (a collection of prompts covering 11 categories).

When applying classifier-free guidance, increasing ÔøΩ may lead to better image-text alignment but worse image fidelity. They found that it is due to train-test mismatch, that is saying, because training data ÔøΩ stays within the range [‚àí1,1], the test data should be so too. Two thresholding strategies are introduced:

* Static thresholding: clip ÔøΩ prediction to [‚àí1,1]
* Dynamic thresholding: at each sampling step, compute ÔøΩ as a certain percentile absolute pixel value; if ÔøΩ>1, clip the prediction to [‚àíÔøΩ,ÔøΩ] and divide by ÔøΩ.

Imagen modifies several designs in U-net to make it *efficient U-Net*.

* Shift model parameters from high resolution blocks to low resolution by adding more residual locks for the lower resolutions;
* Scale the skip connections by 1/2
* Reverse the order of downsampling (move it before convolutions) and upsampling operations (move it after convolution) in order to improve the speed of forward pass.

They found that noise conditioning augmentation, dynamic thresholding and efficient U-Net are critical for image quality, but scaling text encoder size is more important than U-Net size.

### Quick Summary

### 

* **Pros**: Tractability and flexibility are two conflicting objectives in generative modeling. Tractable models can be analytically evaluated and cheaply fit data (e.g. via a Gaussian or Laplace), but they cannot easily describe the structure in rich datasets. Flexible models can fit arbitrary structures in data, but evaluating, training, or sampling from these models is usually expensive. Diffusion models are both analytically tractable and flexible
* 
* **Cons**: Diffusion models rely on a long Markov chain of diffusion steps to generate samples, so it can be quite expensive in terms of time and compute. New methods have been proposed to make the process much faster, but the sampling is still slower than GAN.
* 

### Citation

### 

Cited as:


> *Weng, Lilian. (Jul 2021). What are diffusion models? Lil‚ÄôLog.* [*https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.*](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/.)


>   
> 


>   
> 

Or


```
@article{weng2021diffusion,  

```

```
  

```
### References

### 

[1] Jascha Sohl-Dickstein et al. [‚ÄúDeep Unsupervised Learning using Nonequilibrium Thermodynamics.‚Äù](https://arxiv.org/abs/1503.03585) ICML 2015.

[2] Max Welling & Yee Whye Teh. [‚ÄúBayesian learning via stochastic gradient langevin dynamics.‚Äù](https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf) ICML 2011.

[3] Yang Song & Stefano Ermon. [‚ÄúGenerative modeling by estimating gradients of the data distribution.‚Äù](https://arxiv.org/abs/1907.05600) NeurIPS 2019.

[4] Yang Song & Stefano Ermon. [‚ÄúImproved techniques for training score-based generative models.‚Äù](https://arxiv.org/abs/2006.09011) NeuriPS 2020.

[5] Jonathan Ho et al. [‚ÄúDenoising diffusion probabilistic models.‚Äù](https://arxiv.org/abs/2006.11239) arxiv Preprint arxiv:2006.11239 (2020). [[code](https://github.com/hojonathanho/diffusion)]

[6] Jiaming Song et al. [‚ÄúDenoising diffusion implicit models.‚Äù](https://arxiv.org/abs/2010.02502) arxiv Preprint arxiv:2010.02502 (2020). [[code](https://github.com/ermongroup/ddim)]

[7] Alex Nichol & Prafulla Dhariwal. [‚ÄúImproved denoising diffusion probabilistic models‚Äù](https://arxiv.org/abs/2102.09672) arxiv Preprint arxiv:2102.09672 (2021). [[code](https://github.com/openai/improved-diffusion)]

[8] Prafula Dhariwal & Alex Nichol. [‚ÄúDiffusion Models Beat GANs on Image Synthesis.‚Äù](https://arxiv.org/abs/2105.05233) arxiv Preprint arxiv:2105.05233 (2021). [[code](https://github.com/openai/guided-diffusion)]

[9] Jonathan Ho & Tim Salimans. [‚ÄúClassifier-Free Diffusion Guidance.‚Äù](https://arxiv.org/abs/2207.12598) NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications.

[10] Yang Song, et al. [‚ÄúScore-Based Generative Modeling through Stochastic Differential Equations.‚Äù](https://openreview.net/forum?id=PxTIG12RRHS) ICLR 2021.

[11] Alex Nichol, Prafulla Dhariwal & Aditya Ramesh, et al. [‚ÄúGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.‚Äù](https://arxiv.org/abs/2112.10741) ICML 2022.

[12] Jonathan Ho, et al. [‚ÄúCascaded diffusion models for high fidelity image generation.‚Äù](https://arxiv.org/abs/2106.15282) J. Mach. Learn. Res. 23 (2022): 47‚Äì1.

[13] Aditya Ramesh et al. [‚ÄúHierarchical Text-Conditional Image Generation with CLIP Latents.‚Äù](https://arxiv.org/abs/2204.06125) arxiv Preprint arxiv:2204.06125 (2022).

[14] Chitwan Saharia & William Chan, et al. [‚ÄúPhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding.‚Äù](https://arxiv.org/abs/2205.11487) arxiv Preprint arxiv:2205.11487 (2022).

[15] Rombach & Blattmann, et al. [‚ÄúHigh-Resolution Image Synthesis with Latent Diffusion Models.‚Äù](https://arxiv.org/abs/2112.10752) CVPR 2022.[code](https://github.com/CompVis/latent-diffusion)

* [generative-model](https://lilianweng.github.io/tags/generative-model/)
* [math-heavy](https://lilianweng.github.io/tags/math-heavy/)
* [image-generation](https://lilianweng.github.io/tags/image-generation/)

[¬´  
How to Train Really Large Models on Many GPUs?](https://lilianweng.github.io/posts/2021-09-25-train-large/)[¬ª  
Contrastive Representation Learning](https://lilianweng.github.io/posts/2021-05-31-contrastive/)

¬© 2023 [Lil‚ÄôLog](https://lilianweng.github.io/) Powered by [Hugo](https://gohugo.io/) & [PaperMod](https://git.io/hugopapermod)

  


Also, I will be going to the [LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/) where I hope to learn more best practices on productionize LLMs-powered apps. If you are interested in this topic, stay tuned for my future posts! üòÉ



[View original.](https://medium.com/p/c53da8a2d4c5)

Exported from [Medium](https://medium.com) on May 25, 2024.

